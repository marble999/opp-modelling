{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RangeAgent import EvalAgentDeepRange\n",
    "from PokerRL.game.games import StandardLeduc  # or any other game\n",
    "\n",
    "from DeepCFR.EvalAgentDeepCFR import EvalAgentDeepCFR\n",
    "from DeepCFR.TrainingProfile import TrainingProfile\n",
    "from DeepCFR.workers.driver.Driver import Driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ************************** Initing args for:  DEEP_RANGE_v0   **************************\n"
     ]
    }
   ],
   "source": [
    "t_prof = TrainingProfile(\n",
    "    name=\"DEEP_RANGE_v0\",\n",
    "    nn_type=\"feedforward\",\n",
    "    \n",
    "    max_buffer_size_adv=3e6,\n",
    "    eval_agent_export_freq=20,  # export API to play against the agent\n",
    "    n_traversals_per_iter=1500,\n",
    "    n_batches_adv_training=750,\n",
    "    n_batches_avrg_training=2000,\n",
    "    n_merge_and_table_layer_units_adv=64,\n",
    "    n_merge_and_table_layer_units_avrg=64,\n",
    "    n_units_final_adv=64,\n",
    "    n_units_final_avrg=64,\n",
    "    mini_batch_size_adv=2048,\n",
    "    mini_batch_size_avrg=2048,\n",
    "    init_adv_model=\"last\",\n",
    "    init_avrg_model=\"last\",\n",
    "    use_pre_layers_adv=False,\n",
    "    use_pre_layers_avrg=False,\n",
    "\n",
    "    game_cls=StandardLeduc,\n",
    "\n",
    "    # You can specify one or both modes. Choosing both is useful to compare them.\n",
    "    eval_modes_of_algo=(\n",
    "     EvalAgentDeepCFR.EVAL_MODE_SINGLE,  # SD-CFR\n",
    "     EvalAgentDeepCFR.EVAL_MODE_AVRG_NET,  # Deep CFR\n",
    "    ),\n",
    "\n",
    "    DISTRIBUTED=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeActionNet(\n",
       "  (range_net): RangeNet(\n",
       "    (_relu): ReLU()\n",
       "    (_mpm): MainPokerModuleFLAT(\n",
       "      (_relu): ReLU()\n",
       "      (final_fc_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (final_fc_2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (_final_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (_out_layer): Linear(in_features=64, out_features=6, bias=True)\n",
       "    (_softmax): Softmax()\n",
       "  )\n",
       "  (_relu): ReLU()\n",
       "  (_final_layer): Linear(in_features=27, out_features=64, bias=True)\n",
       "  (_out_layer): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvalAgentDeepRange(t_prof, mode=None, device=None).policy._net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "range_loss = nn.CrossEntropyLoss()\n",
    "action_loss = nn.MSELoss() ## cross-entropy would be ideal\n",
    "\n",
    "def hole_card_onehot(hole_card):\n",
    "    rank = hole_card[0][0]\n",
    "    suit = hole_card[0][1]\n",
    "    out = rank + suit * 3 ## arbitrary but it will learn the relationship\n",
    "    return torch.LongTensor([out])\n",
    "\n",
    "def distill(student_agent, teacher_agent, args={'lr':1e-2, 'iters':10000, 'lambda':10}):\n",
    "    \"\"\"\n",
    "    Distill student_agent to play like teacher_agent\n",
    "    \"\"\"\n",
    "    \n",
    "    env_bldr = student_agent.env_bldr\n",
    "    env_cls = env_bldr.env_cls\n",
    "    env_args = env_bldr.env_args\n",
    "    lut_holder = env_cls.get_lut_holder()\n",
    "    \n",
    "    assert(student_agent.env_bldr.env_cls == teacher_agent.env_bldr.env_cls)\n",
    "    assert(env_args.n_seats == 2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(student_agent.policy._net.parameters()), lr=args['lr'])\n",
    "    start_time = time.time()\n",
    "    \n",
    "    REFERENCE_AGENT = 0\n",
    "    \n",
    "    _env = env_cls(env_args=env_args, lut_holder=lut_holder, is_evaluating=True)\n",
    "    _eval_agents = [teacher_agent, deepcopy(teacher_agent)]\n",
    "    \n",
    "    results = {\n",
    "        \"range_loss\": [],\n",
    "        \"action_loss\": [],\n",
    "        \"total_loss\": [],\n",
    "    }\n",
    "    iters = 0 # number of hands played\n",
    "    evals = 0 # number of teaching moments\n",
    "    \n",
    "    # zero grads, set net to train mode\n",
    "    student_agent.policy._net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    while iters < args['iters']:\n",
    "        iters += 1\n",
    "        \n",
    "        if iters % 200 == 0:\n",
    "            print(\"Iters {} | Evals {} | RangeLoss {} | ActionLoss {} | TotalLoss {}\".format(\n",
    "                iters, evals, sum(results['range_loss']) / evals, sum(results['action_loss']) / evals, sum(results['total_loss']) / evals\n",
    "            ))\n",
    "            \n",
    "            # print(\"gradient:\", list(student_agent.policy._net.parameters())[0].grad)\n",
    "\n",
    "            # print(\"old params:\", list(student_agent.policy._net.parameters())[0])\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(\"new params:\", list(student_agent.policy._net.parameters())[0])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        for seat_p0 in range(_env.N_SEATS):\n",
    "            seat_p1 = 1 - seat_p0\n",
    "            \n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Reset Episode\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            _, r_for_all, done, info = _env.reset()\n",
    "            for e in _eval_agents + [student_agent]:\n",
    "                e.reset(deck_state_dict=_env.cards_state_dict())\n",
    "\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Play Episode\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "            while not done:\n",
    "                p_id_acting = _env.current_player.seat_id\n",
    "\n",
    "                if p_id_acting == seat_p0:\n",
    "                    evals += 1 #increment counter\n",
    "                    \n",
    "                    # set student to position of agent 1, estimate range + actions\n",
    "                    student_agent.set_env_wrapper(_eval_agents[REFERENCE_AGENT]._internal_env_wrapper) \n",
    "                    student_a_probs = student_agent.get_a_probs_tensor()\n",
    "                    student_range_probs = student_agent.get_range_probs()\n",
    "                    \n",
    "                    # get true values \n",
    "                    a_probs = torch.Tensor(_eval_agents[REFERENCE_AGENT].get_a_probs())\n",
    "                    range_label = _env.get_hole_cards_of_player(seat_p1) #get opponent's true range\n",
    "                    range_label = hole_card_onehot(range_label) # convert to label\n",
    "                    action_int, _ = _eval_agents[REFERENCE_AGENT].get_action(step_env=True, need_probs=False)\n",
    "                    \n",
    "                    # print(\"True:\", a_probs, range_label)\n",
    "                    # print(\"Prediction:\", student_a_probs, student_range_probs)\n",
    "                    # print(\"Checking requires_grad:\", student_a_probs.requires_grad, student_range_probs.requires_grad)\n",
    "                    \n",
    "                    # compute loss\n",
    "                    rloss = range_loss(student_range_probs.view(1,-1), range_label)\n",
    "                    aloss = action_loss(student_a_probs, a_probs)\n",
    "                    loss = rloss + args['lambda'] * aloss\n",
    "                    \n",
    "                    results['total_loss'].append(loss)\n",
    "                    results['range_loss'].append(rloss)\n",
    "                    results['action_loss'].append(aloss)\n",
    "                    \n",
    "                    # print(\"Loss:\", rloss, aloss, loss)\n",
    "                    \n",
    "                    # backpropogate\n",
    "                    loss.backward() # accumulate gradients over many steps\n",
    "                                        \n",
    "                    # notify opponent\n",
    "                    _eval_agents[1 - REFERENCE_AGENT].notify_of_action(p_id_acted=p_id_acting,\n",
    "                                                                       action_he_did=action_int)\n",
    "                elif p_id_acting == seat_p1:\n",
    "                    a_probs = _eval_agents[REFERENCE_AGENT].get_a_probs()\n",
    "                    action_int, _ = _eval_agents[1 - REFERENCE_AGENT].get_action(step_env=True, need_probs=False)\n",
    "                    _eval_agents[REFERENCE_AGENT].notify_of_action(p_id_acted=p_id_acting,\n",
    "                                                                   action_he_did=action_int)\n",
    "                else:\n",
    "                    raise ValueError(\"Only HU supported!\")\n",
    "                \n",
    "                _, r_for_all, done, info = _env.step(action_int)  \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Time taken\", end_time - start_time)\n",
    "\n",
    "    print(optimizer)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters 200 | Evals 828 | RangeLoss 1.7923096418380737 | ActionLoss 0.07373639196157455 | TotalLoss 1.7996827363967896\n",
      "Iters 400 | Evals 1665 | RangeLoss 1.7918970584869385 | ActionLoss 0.0715893805027008 | TotalLoss 1.799059271812439\n",
      "Iters 600 | Evals 2501 | RangeLoss 1.78530752658844 | ActionLoss 0.0700935646891594 | TotalLoss 1.7923182249069214\n",
      "Iters 800 | Evals 3338 | RangeLoss 1.780920386314392 | ActionLoss 0.06919553130865097 | TotalLoss 1.7878450155258179\n",
      "Iters 1000 | Evals 4174 | RangeLoss 1.7761164903640747 | ActionLoss 0.06778756529092789 | TotalLoss 1.782900333404541\n",
      "Iters 1200 | Evals 5006 | RangeLoss 1.7724326848983765 | ActionLoss 0.06747311353683472 | TotalLoss 1.7791866064071655\n",
      "Iters 1400 | Evals 5828 | RangeLoss 1.767167091369629 | ActionLoss 0.06657982617616653 | TotalLoss 1.773828387260437\n",
      "Iters 1600 | Evals 6651 | RangeLoss 1.76438307762146 | ActionLoss 0.0659555122256279 | TotalLoss 1.7709835767745972\n",
      "Iters 1800 | Evals 7470 | RangeLoss 1.7620341777801514 | ActionLoss 0.06538593024015427 | TotalLoss 1.768578290939331\n",
      "Iters 2000 | Evals 8296 | RangeLoss 1.7584877014160156 | ActionLoss 0.06481435149908066 | TotalLoss 1.76497220993042\n",
      "Iters 2200 | Evals 9136 | RangeLoss 1.755605936050415 | ActionLoss 0.06431867182254791 | TotalLoss 1.7620428800582886\n",
      "Iters 2400 | Evals 9943 | RangeLoss 1.753497838973999 | ActionLoss 0.06400950998067856 | TotalLoss 1.759901523590088\n",
      "Iters 2600 | Evals 10779 | RangeLoss 1.7510446310043335 | ActionLoss 0.06333379447460175 | TotalLoss 1.7573870420455933\n",
      "Iters 2800 | Evals 11640 | RangeLoss 1.7487608194351196 | ActionLoss 0.0630011186003685 | TotalLoss 1.7550690174102783\n",
      "Iters 3000 | Evals 12488 | RangeLoss 1.7473183870315552 | ActionLoss 0.06269298493862152 | TotalLoss 1.753596305847168\n",
      "Iters 3200 | Evals 13344 | RangeLoss 1.7465131282806396 | ActionLoss 0.06258353590965271 | TotalLoss 1.7527821063995361\n",
      "Iters 3400 | Evals 14160 | RangeLoss 1.7457066774368286 | ActionLoss 0.062222402542829514 | TotalLoss 1.7519376277923584\n",
      "Iters 3600 | Evals 14979 | RangeLoss 1.7445008754730225 | ActionLoss 0.0620773620903492 | TotalLoss 1.7507187128067017\n",
      "Iters 3800 | Evals 15849 | RangeLoss 1.7427139282226562 | ActionLoss 0.061707306653261185 | TotalLoss 1.7488930225372314\n",
      "Iters 4000 | Evals 16689 | RangeLoss 1.7425487041473389 | ActionLoss 0.06155760958790779 | TotalLoss 1.7487130165100098\n",
      "Iters 4200 | Evals 17550 | RangeLoss 1.7411271333694458 | ActionLoss 0.06115299090743065 | TotalLoss 1.7472524642944336\n",
      "Iters 4400 | Evals 18409 | RangeLoss 1.7404283285140991 | ActionLoss 0.060903146862983704 | TotalLoss 1.7465243339538574\n",
      "Iters 4600 | Evals 19272 | RangeLoss 1.739822268486023 | ActionLoss 0.06062820553779602 | TotalLoss 1.7458946704864502\n",
      "Iters 4800 | Evals 20132 | RangeLoss 1.7387545108795166 | ActionLoss 0.06030041351914406 | TotalLoss 1.7447938919067383\n",
      "Iters 5000 | Evals 20977 | RangeLoss 1.7377170324325562 | ActionLoss 0.06000018119812012 | TotalLoss 1.7437270879745483\n",
      "Iters 5200 | Evals 21795 | RangeLoss 1.7373071908950806 | ActionLoss 0.05988948792219162 | TotalLoss 1.7433069944381714\n",
      "Iters 5400 | Evals 22637 | RangeLoss 1.7365009784698486 | ActionLoss 0.059609100222587585 | TotalLoss 1.7424770593643188\n",
      "Iters 5600 | Evals 23458 | RangeLoss 1.7364753484725952 | ActionLoss 0.0593300424516201 | TotalLoss 1.7424181699752808\n",
      "Iters 5800 | Evals 24306 | RangeLoss 1.7360299825668335 | ActionLoss 0.0589064322412014 | TotalLoss 1.7419260740280151\n",
      "Iters 6000 | Evals 25140 | RangeLoss 1.7357362508773804 | ActionLoss 0.0585489422082901 | TotalLoss 1.7415889501571655\n",
      "Iters 6200 | Evals 25971 | RangeLoss 1.7351688146591187 | ActionLoss 0.058200281113386154 | TotalLoss 1.7409852743148804\n",
      "Iters 6400 | Evals 26816 | RangeLoss 1.7346407175064087 | ActionLoss 0.05792343616485596 | TotalLoss 1.7404265403747559\n",
      "Iters 6600 | Evals 27663 | RangeLoss 1.734623670578003 | ActionLoss 0.05763738974928856 | TotalLoss 1.7403806447982788\n",
      "Iters 6800 | Evals 28506 | RangeLoss 1.7344645261764526 | ActionLoss 0.05738995596766472 | TotalLoss 1.7401983737945557\n",
      "Iters 7000 | Evals 29321 | RangeLoss 1.7336788177490234 | ActionLoss 0.05712287873029709 | TotalLoss 1.7393779754638672\n",
      "Iters 7200 | Evals 30144 | RangeLoss 1.7334678173065186 | ActionLoss 0.056827884167432785 | TotalLoss 1.739135980606079\n",
      "Iters 7400 | Evals 30961 | RangeLoss 1.7331008911132812 | ActionLoss 0.05646682158112526 | TotalLoss 1.7387363910675049\n",
      "Iters 7600 | Evals 31797 | RangeLoss 1.7325267791748047 | ActionLoss 0.05614178627729416 | TotalLoss 1.738129734992981\n",
      "Iters 7800 | Evals 32624 | RangeLoss 1.7321977615356445 | ActionLoss 0.05594124644994736 | TotalLoss 1.7377820014953613\n",
      "Iters 8000 | Evals 33433 | RangeLoss 1.7320948839187622 | ActionLoss 0.055662501603364944 | TotalLoss 1.7376508712768555\n",
      "Iters 8200 | Evals 34229 | RangeLoss 1.731841802597046 | ActionLoss 0.05536957085132599 | TotalLoss 1.737369418144226\n",
      "Iters 8400 | Evals 35088 | RangeLoss 1.7319436073303223 | ActionLoss 0.05496726930141449 | TotalLoss 1.7374284267425537\n",
      "Iters 8600 | Evals 35927 | RangeLoss 1.7313412427902222 | ActionLoss 0.054674915969371796 | TotalLoss 1.7367967367172241\n",
      "Iters 8800 | Evals 36745 | RangeLoss 1.7312979698181152 | ActionLoss 0.054408296942710876 | TotalLoss 1.7367281913757324\n",
      "Iters 9000 | Evals 37567 | RangeLoss 1.7308895587921143 | ActionLoss 0.05404084175825119 | TotalLoss 1.736283540725708\n",
      "Iters 9200 | Evals 38410 | RangeLoss 1.7305324077606201 | ActionLoss 0.05369545519351959 | TotalLoss 1.735897421836853\n",
      "Iters 9400 | Evals 39251 | RangeLoss 1.7305880784988403 | ActionLoss 0.0533900186419487 | TotalLoss 1.7359285354614258\n",
      "Iters 9600 | Evals 40086 | RangeLoss 1.7304129600524902 | ActionLoss 0.05306330695748329 | TotalLoss 1.735716700553894\n",
      "Iters 9800 | Evals 40921 | RangeLoss 1.7302007675170898 | ActionLoss 0.05287625640630722 | TotalLoss 1.7354775667190552\n",
      "Iters 10000 | Evals 41728 | RangeLoss 1.730224609375 | ActionLoss 0.05253543332219124 | TotalLoss 1.735467791557312\n",
      "Time taken 293.2564618587494\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Distillation\n",
    "\n",
    "agent_file1 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/120/eval_agentAVRG_NET.pkl\"\n",
    "\n",
    "student_agent = EvalAgentDeepRange(t_prof, mode=None, device=None)\n",
    "teacher_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file1)\n",
    "\n",
    "results = distill(student_agent, teacher_agent, args={'lr':1e-2, 'iters':10000, 'lambda':0.1})\n",
    "\n",
    "# student_agent.save_to_file(\"deep_range_example.pt\")\n",
    "# student_agent.load_from_file(\"deep_range_example.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1: student\n",
      "Agent 2: teacher\n",
      "\n",
      "Played 2000 hands of poker.\n",
      "Player  DEEPRANGE: -136.0 +/- 157.56683117557216\n",
      "Player  AVRG_NET: 136.0 +/- 157.56683117557216\n",
      "\n",
      "\n",
      "Agent 1: student\n",
      "Agent 2: crappy\n",
      "\n",
      "Played 2000 hands of poker.\n",
      "Player  DEEPRANGE: 302.0 +/- 190.76949881451117\n",
      "Player  AVRG_NET: -302.0 +/- 190.76949881451117\n",
      "\n",
      "\n",
      "Agent 1: crappy\n",
      "Agent 2: teacher\n",
      "\n",
      "Played 2000 hands of poker.\n",
      "Player  AVRG_NET: -836.5 +/- 200.8964947712741\n",
      "Player  AVRG_NET: 836.5 +/- 200.8964947712741\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluation\n",
    "\n",
    "import time, sys\n",
    "from os.path import dirname, abspath\n",
    "\n",
    "sys.path.append(\"/home/leduc/Deep-CFR/\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from DeepCFR.EvalAgentDeepCFR import EvalAgentDeepCFR\n",
    "from PokerRL.game.AgentTournament import AgentTournament\n",
    "\n",
    "agent_file1 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/120/eval_agentAVRG_NET.pkl\"\n",
    "agent_file2 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_2/2/eval_agentAVRG_NET.pkl\"\n",
    "\n",
    "teacher_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file1)\n",
    "crappy_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file2)\n",
    "\n",
    "# student_file = \"deep_range_100000_0.01.pt\"\n",
    "# student_agent.load_from_file(student_file)\n",
    "\n",
    "def h2heval(eval_agent_1, eval_agent_2, names=['agent1', 'agent2']):\n",
    "    env_bldr = eval_agent_1.env_bldr\n",
    "    env = env_bldr.get_new_env(is_evaluating=False)\n",
    "    env_cls = env_bldr.env_cls\n",
    "    env_args = env_bldr.env_args\n",
    "    \n",
    "    print(\"Agent 1:\", names[0])\n",
    "    print(\"Agent 2:\", names[1])\n",
    "    matchup = AgentTournament(env_cls, env_args, eval_agent_1, eval_agent_2)\n",
    "    mean, upper_conf95, lower_conf95 = matchup.run(n_games_per_seat=1000)\n",
    "    print(\"\\n\")\n",
    "\n",
    "h2heval(student_agent, teacher_agent, names=['student', 'teacher'])\n",
    "h2heval(student_agent, crappy_agent, names=['student', 'crappy'])\n",
    "h2heval(crappy_agent, teacher_agent, names=['crappy', 'teacher'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
