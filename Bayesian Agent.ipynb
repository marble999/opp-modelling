{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ************************** Initing args for:  DEEPCOPY_v0   **************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bayesian PsuedoCount updating\n",
    "\"\"\"\n",
    "\n",
    "from RangeAgent import EvalAgentDeepRange\n",
    "from TreeAgent import EvalAgentTree\n",
    "\n",
    "from PokerRL.game.games import StandardLeduc  # or any other game\n",
    "from PokerRL.eval.rl_br.RLBRArgs import RLBRArgs\n",
    "\n",
    "from DeepCFR.EvalAgentDeepCFR import EvalAgentDeepCFR\n",
    "from DeepCFR.TrainingProfile import TrainingProfile\n",
    "from DeepCFR.workers.driver.Driver import Driver\n",
    "\n",
    "rlbr_args = RLBRArgs(\n",
    "    rlbr_bet_set = None\n",
    ")\n",
    "\n",
    "t_prof = TrainingProfile(\n",
    "    name=\"DEEPCOPY_v0\",\n",
    "    nn_type=\"feedforward\",\n",
    "    \n",
    "    max_buffer_size_adv=3e6,\n",
    "    eval_agent_export_freq=20,  # export API to play against the agent\n",
    "    n_traversals_per_iter=1500,\n",
    "    n_batches_adv_training=750,\n",
    "    n_batches_avrg_training=2000,\n",
    "    n_merge_and_table_layer_units_adv=64,\n",
    "    n_merge_and_table_layer_units_avrg=64,\n",
    "    n_units_final_adv=64,\n",
    "    n_units_final_avrg=64,\n",
    "    mini_batch_size_adv=2048,\n",
    "    mini_batch_size_avrg=2048,\n",
    "    init_adv_model=\"last\",\n",
    "    init_avrg_model=\"last\",\n",
    "    use_pre_layers_adv=False,\n",
    "    use_pre_layers_avrg=False,\n",
    "\n",
    "    game_cls=StandardLeduc,\n",
    "\n",
    "    # You can specify one or both modes. Choosing both is useful to compare them.\n",
    "    eval_modes_of_algo=(\n",
    "     # EvalAgentDeepCFR.EVAL_MODE_SINGLE,  # SD-CFR\n",
    "     EvalAgentDeepCFR.EVAL_MODE_AVRG_NET,  # Deep-CFR\n",
    "    ),\n",
    "\n",
    "    DISTRIBUTED=False,\n",
    "    rl_br_args=rlbr_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "action_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def best_response(agent):\n",
    "    \"\"\"\n",
    "    Returns strategy that is best response to agent strategy\n",
    "    \"\"\"\n",
    "    br = EvalAgentTree(t_prof, br_agent=agent, mode=None, device=None)\n",
    "    br.mode = \"BR\"\n",
    "    return br\n",
    "\n",
    "def bayesian_while_play(bayesian_agent, enemy_agent, args={'lr':1e-2, 'iters':10000}):\n",
    "    \"\"\"\n",
    "    Train bayesian_agent to mimic stationery enemy_agent.\n",
    "    \"\"\"\n",
    "            \n",
    "    env_bldr = bayesian_agent.env_bldr\n",
    "    env_cls = env_bldr.env_cls\n",
    "    env_args = env_bldr.env_args\n",
    "    lut_holder = env_cls.get_lut_holder()\n",
    "    \n",
    "    assert(bayesian_agent.env_bldr.env_cls == enemy_agent.env_bldr.env_cls)\n",
    "    assert(env_args.n_seats == 2)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    REFERENCE_AGENT = 0\n",
    "    \n",
    "    _env = env_cls(env_args=env_args, lut_holder=lut_holder, is_evaluating=True)\n",
    "    _eval_agents = [enemy_agent, deepcopy(bayesian_agent)] # play against an new frozen copy of the BR to agent while training\n",
    "    \n",
    "    results = {\n",
    "        \"winnings\": []\n",
    "    }\n",
    "    iters = 0 # number of hands played\n",
    "    evals = 0 # number of teaching moments\n",
    "\n",
    "    while iters < args['iters']:\n",
    "        iters += 1\n",
    "        \n",
    "        if iters % 200 == 0:\n",
    "            print(\"Iters {} | Evals {} | Winnings mBB/Hand {} | \".format(\n",
    "                iters, evals, sum(results[\"winnings\"]) / iters\n",
    "            ))\n",
    "            \n",
    "            # play against an new frozen copy of the BR to agent while training\n",
    "            _eval_agents[1] = best_response(deepcopy(bayesian_agent)) \n",
    "        \n",
    "        for seat_p0 in range(_env.N_SEATS):\n",
    "            seat_p1 = 1 - seat_p0\n",
    "            \n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Reset Episode\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            _, r_for_all, done, info = _env.reset()\n",
    "            for e in _eval_agents:\n",
    "                e.reset(deck_state_dict=_env.cards_state_dict())\n",
    "\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Play Episode\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "            while not done:\n",
    "                p_id_acting = _env.current_player.seat_id\n",
    "\n",
    "                if p_id_acting == seat_p0:\n",
    "                    evals += 1 #increment counter\n",
    "                    \n",
    "                    # set bayesian agent to position of agent 1, update psuedocount\n",
    "                    bayesian_agent.set_env_wrapper(_eval_agents[REFERENCE_AGENT]._internal_env_wrapper)\n",
    "                    node = bayesian_agent._find_node_by_env(bayesian_agent._internal_env_wrapper._action_history_list)\n",
    "                    range_idx = bayesian_agent._internal_env_wrapper.env.get_range_idx(p_id=p_id_acting) #get opponent hole card\n",
    "                    \n",
    "                    # get true values \n",
    "                    action_int, _ = _eval_agents[REFERENCE_AGENT].get_action(step_env=True, need_probs=False)\n",
    "                    \n",
    "                    # update pseudocount\n",
    "                    node.data[range_idx, action_int] += 1 #node.allowed_actions.index(action_int)\n",
    "                    \n",
    "                    # notify opponent\n",
    "                    _eval_agents[1 - REFERENCE_AGENT].notify_of_action(p_id_acted=p_id_acting,\n",
    "                                                                       action_he_did=action_int)\n",
    "                    \n",
    "                elif p_id_acting == seat_p1:\n",
    "                    action_int, _ = _eval_agents[1 - REFERENCE_AGENT].get_action(step_env=True,\n",
    "                                                                                 need_probs=False)\n",
    "                    _eval_agents[REFERENCE_AGENT].notify_of_action(p_id_acted=p_id_acting,\n",
    "                                                                   action_he_did=action_int)\n",
    "                else:\n",
    "                    raise ValueError(\"Only HU supported!\")\n",
    "\n",
    "                _, r_for_all, done, info = _env.step(action_int)\n",
    "                \n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Add Rews\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            results[\"winnings\"].append(r_for_all[seat_p0] * _env.REWARD_SCALAR * _env.EV_NORMALIZER)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Time taken\", end_time - start_time)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.ones((3,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters 200 | Evals 843 | Winnings mBB/Hand 1655.0 | \n",
      "Iters 400 | Evals 1772 | Winnings mBB/Hand 1217.5 | \n",
      "Iters 600 | Evals 2644 | Winnings mBB/Hand 1020.0 | \n",
      "Iters 800 | Evals 3479 | Winnings mBB/Hand 895.0 | \n",
      "Iters 1000 | Evals 4325 | Winnings mBB/Hand 692.0 | \n",
      "Iters 1200 | Evals 5179 | Winnings mBB/Hand 640.8333333333334 | \n",
      "Iters 1400 | Evals 6062 | Winnings mBB/Hand 623.5714285714286 | \n",
      "Iters 1600 | Evals 6970 | Winnings mBB/Hand 423.75 | \n",
      "Iters 1800 | Evals 7840 | Winnings mBB/Hand 452.77777777777777 | \n",
      "Iters 2000 | Evals 8622 | Winnings mBB/Hand 431.0 | \n",
      "Iters 2200 | Evals 9438 | Winnings mBB/Hand 432.27272727272725 | \n",
      "Iters 2400 | Evals 10265 | Winnings mBB/Hand 308.3333333333333 | \n",
      "Iters 2600 | Evals 11116 | Winnings mBB/Hand 262.6923076923077 | \n",
      "Iters 2800 | Evals 12010 | Winnings mBB/Hand 167.5 | \n",
      "Iters 3000 | Evals 12888 | Winnings mBB/Hand 134.0 | \n",
      "Iters 3200 | Evals 13768 | Winnings mBB/Hand 72.8125 | \n",
      "Iters 3400 | Evals 14667 | Winnings mBB/Hand 34.411764705882355 | \n",
      "Iters 3600 | Evals 15529 | Winnings mBB/Hand 44.44444444444444 | \n",
      "Iters 3800 | Evals 16366 | Winnings mBB/Hand 27.894736842105264 | \n",
      "Iters 4000 | Evals 17227 | Winnings mBB/Hand 5.25 | \n",
      "Iters 4200 | Evals 18048 | Winnings mBB/Hand 4.761904761904762 | \n",
      "Iters 4400 | Evals 18932 | Winnings mBB/Hand -29.318181818181817 | \n",
      "Iters 4600 | Evals 19758 | Winnings mBB/Hand -71.73913043478261 | \n",
      "Iters 4800 | Evals 20616 | Winnings mBB/Hand -134.16666666666666 | \n",
      "Iters 5000 | Evals 21449 | Winnings mBB/Hand -148.6 | \n",
      "Iters 5200 | Evals 22283 | Winnings mBB/Hand -153.84615384615384 | \n",
      "Iters 5400 | Evals 23150 | Winnings mBB/Hand -164.25925925925927 | \n",
      "Iters 5600 | Evals 23933 | Winnings mBB/Hand -168.75 | \n",
      "Iters 5800 | Evals 24759 | Winnings mBB/Hand -185.17241379310346 | \n",
      "Iters 6000 | Evals 25591 | Winnings mBB/Hand -211.16666666666666 | \n",
      "Iters 6200 | Evals 26416 | Winnings mBB/Hand -217.09677419354838 | \n",
      "Iters 6400 | Evals 27238 | Winnings mBB/Hand -240.15625 | \n",
      "Iters 6600 | Evals 28073 | Winnings mBB/Hand -255.45454545454547 | \n",
      "Iters 6800 | Evals 28886 | Winnings mBB/Hand -272.05882352941177 | \n",
      "Iters 7000 | Evals 29679 | Winnings mBB/Hand -284.0 | \n",
      "Iters 7200 | Evals 30519 | Winnings mBB/Hand -281.6666666666667 | \n",
      "Iters 7400 | Evals 31326 | Winnings mBB/Hand -289.72972972972974 | \n",
      "Iters 7600 | Evals 32106 | Winnings mBB/Hand -300.7894736842105 | \n",
      "Iters 7800 | Evals 32898 | Winnings mBB/Hand -301.9230769230769 | \n",
      "Iters 8000 | Evals 33694 | Winnings mBB/Hand -296.5 | \n",
      "Iters 8200 | Evals 34522 | Winnings mBB/Hand -295.3658536585366 | \n",
      "Iters 8400 | Evals 35357 | Winnings mBB/Hand -316.3095238095238 | \n",
      "Iters 8600 | Evals 36201 | Winnings mBB/Hand -323.72093023255815 | \n",
      "Iters 8800 | Evals 37049 | Winnings mBB/Hand -334.09090909090907 | \n",
      "Iters 9000 | Evals 37881 | Winnings mBB/Hand -338.8888888888889 | \n",
      "Iters 9200 | Evals 38737 | Winnings mBB/Hand -344.0217391304348 | \n",
      "Iters 9400 | Evals 39567 | Winnings mBB/Hand -346.59574468085106 | \n",
      "Iters 9600 | Evals 40329 | Winnings mBB/Hand -341.875 | \n",
      "Iters 9800 | Evals 41145 | Winnings mBB/Hand -342.85714285714283 | \n",
      "Iters 10000 | Evals 41976 | Winnings mBB/Hand -351.7 | \n",
      "Time taken 50.53241539001465\n"
     ]
    }
   ],
   "source": [
    "agent_file1 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/120/eval_agentAVRG_NET.pkl\"\n",
    "agent_file2 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/20/eval_agentAVRG_NET.pkl\"\n",
    "\n",
    "enemy_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file2)\n",
    "init_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file1)\n",
    "bayesian_agent = EvalAgentTree(t_prof, br_agent=init_agent, mode=\"BAYESIAN\", device=None)\n",
    "\n",
    "results = bayesian_while_play(bayesian_agent, enemy_agent, args={'lr':1e-2, 'iters':10000})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TreeAgent.EvalAgentTree at 0x7fd2be28e390>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
