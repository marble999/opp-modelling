{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ************************** Initing args for:  DEEPCOPY_v0   **************************\n"
     ]
    }
   ],
   "source": [
    "from RangeAgent import EvalAgentDeepRange\n",
    "from PokerRL.game.games import StandardLeduc  # or any other game\n",
    "from PokerRL.eval.rl_br.RLBRArgs import RLBRArgs\n",
    "\n",
    "from DeepCFR.EvalAgentDeepCFR import EvalAgentDeepCFR\n",
    "from DeepCFR.TrainingProfile import TrainingProfile\n",
    "from DeepCFR.workers.driver.Driver import Driver\n",
    "\n",
    "from PokerRL.game._.tree.PublicTree import PublicTree\n",
    "from PokerRL.rl import rl_util\n",
    "\n",
    "rlbr_args = RLBRArgs(\n",
    "    rlbr_bet_set = None\n",
    ")\n",
    "\n",
    "t_prof = TrainingProfile(\n",
    "    name=\"DEEPCOPY_v0\",\n",
    "    nn_type=\"feedforward\",\n",
    "    \n",
    "    max_buffer_size_adv=3e6,\n",
    "    eval_agent_export_freq=20,  # export API to play against the agent\n",
    "    n_traversals_per_iter=1500,\n",
    "    n_batches_adv_training=750,\n",
    "    n_batches_avrg_training=2000,\n",
    "    n_merge_and_table_layer_units_adv=64,\n",
    "    n_merge_and_table_layer_units_avrg=64,\n",
    "    n_units_final_adv=64,\n",
    "    n_units_final_avrg=64,\n",
    "    mini_batch_size_adv=2048,\n",
    "    mini_batch_size_avrg=2048,\n",
    "    init_adv_model=\"last\",\n",
    "    init_avrg_model=\"last\",\n",
    "    use_pre_layers_adv=False,\n",
    "    use_pre_layers_avrg=False,\n",
    "\n",
    "    game_cls=StandardLeduc,\n",
    "\n",
    "    # You can specify one or both modes. Choosing both is useful to compare them.\n",
    "    eval_modes_of_algo=(\n",
    "     # EvalAgentDeepCFR.EVAL_MODE_SINGLE,  # SD-CFR\n",
    "     EvalAgentDeepCFR.EVAL_MODE_AVRG_NET,  # Deep-CFR\n",
    "    ),\n",
    "\n",
    "    DISTRIBUTED=False,\n",
    "    rl_br_args=rlbr_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/leduc/Deep-CFR/\")\n",
    "sys.path.append(\"/home/leduc/PokerRL/\")\n",
    "\n",
    "from PokerRL.game import Poker\n",
    "from PokerRL.game._.tree._.nodes import PlayerActionNode\n",
    "from PokerRL.rl import rl_util\n",
    "from PokerRL.rl.base_cls.EvalAgentBase import EvalAgentBase as _EvalAgentBase\n",
    "from PokerRL.rl.errors import UnknownModeError\n",
    "\n",
    "from DeepCFR.IterationStrategy import IterationStrategy\n",
    "from DeepCFR.StrategyBuffer import StrategyBuffer\n",
    "from DeepCFR.workers.la.AvrgWrapper import AvrgWrapper\n",
    "\n",
    "from RangeWrapper import RangeWrapper\n",
    "\n",
    "NP_FLOAT_TYPE = np.float64  # Use 64 for extra stability in big games\n",
    "\n",
    "class EvalAgentTree(_EvalAgentBase):\n",
    "    \n",
    "    def __init__(self, t_prof, br_agent, mode=None, device=None):\n",
    "        super().__init__(t_prof=t_prof, mode=mode, device=device)\n",
    "        \n",
    "        self.tree = PublicTree(\n",
    "            env_bldr=rl_util.get_env_builder(t_prof=t_prof),\n",
    "            stack_size=t_prof.eval_stack_sizes[0],\n",
    "            stop_at_street=None,\n",
    "            put_out_new_round_after_limit=True,\n",
    "            is_debugging=t_prof.DEBUGGING\n",
    "        )\n",
    "        self.tree.build_tree()\n",
    "        self.br_agent = br_agent # agent to play best response against\n",
    "        self.solve_br()\n",
    "        \n",
    "        self.modes = [\"EVAL\", \"BR\"]\n",
    "        self.mode = \"EVAL\" # default is eval\n",
    "        \n",
    "    def can_compute_mode(self):\n",
    "        \"\"\" All modes are always computable (i.e. not dependent on iteration etc.)\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def _find_node_by_env(self, action_history):\n",
    "        node = self.tree.root\n",
    "        \n",
    "        \"\"\"\n",
    "        envw = self._internal_env_wrapper.env_bldr\n",
    "        i = 0\n",
    "        \n",
    "        last_round_ = None\n",
    "        round_ = node.env_state['current_round']\n",
    "        p_id = node.p_id_acting_next\n",
    "        \n",
    "        if last_round_ == round_:\n",
    "            nth_action_this_round += 1\n",
    "        else:\n",
    "            last_round_ = round_\n",
    "            nth_action_this_round = 0\n",
    "        \n",
    "        def reverse_idx(idx, round_, p_id, nth_action_this_round):\n",
    "            return i - nth_action_this_round - p_id * envw._VEC_HALF_ROUND_SIZE[round_] - envw._VEC_ROUND_OFFSETS[round_]\n",
    "        \n",
    "        while i < len(action_history):\n",
    "            if action_history[i] == 1:\n",
    "                action_idx = reverse_idx(i, round_, p_id, nth_action_this_round) + 1 # fold never accepted\n",
    "                node = node.children[action_idx] #recurse through tree\n",
    "        \"\"\"\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < len(action_history):\n",
    "            if isinstance(node.children[0], PlayerActionNode): #next node is playerAction\n",
    "                action = action_history[i][0]\n",
    "                assert(node.p_id_acting_next == action_history[i][2])\n",
    "                node = node.children[node.allowed_actions.index(action)]\n",
    "                i += 1\n",
    "            else: #chance node, flop\n",
    "                assert(node.children[0].action == \"CHANCE\")\n",
    "                card = self._internal_env_wrapper.env.board\n",
    "                node = node.children[self._card_to_idx(card)]\n",
    "                assert(self._card_to_idx(node.env_state['board_2d']) == self._card_to_idx(card))\n",
    "                \n",
    "        if not isinstance(node.children[0], PlayerActionNode): # just need to do one more loop lol\n",
    "            assert(node.children[0].action == \"CHANCE\")\n",
    "            card = self._internal_env_wrapper.env.board\n",
    "            node = node.children[self._card_to_idx(card)]\n",
    "            assert(self._card_to_idx(node.env_state['board_2d']) == self._card_to_idx(card))\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _card_to_idx(self, card):\n",
    "        return card[0][0] * 2 + card[0][1]\n",
    "    \n",
    "    def solve_br(self):\n",
    "        self.tree.fill_with_agent_policy(agent=self.br_agent)\n",
    "        self.tree.compute_ev()\n",
    "                    \n",
    "    def get_action(self, step_env=True, need_probs=False):\n",
    "        \"\"\" !! BEFORE CALLING, NOTIFY EVALAGENT OF THE PAST ACTIONS / ACTIONSEQUENCE !! \"\"\"\n",
    "        # print(\"action history\", self._internal_env_wrapper._action_history_vector)\n",
    "        # node = self._find_node_by_env(self._internal_env_wrapper._action_history_vector)\n",
    "        \n",
    "        # print(\"action history\", self._internal_env_wrapper._action_history_list)\n",
    "        node = self._find_node_by_env(self._internal_env_wrapper._action_history_list)\n",
    "        \n",
    "        p_id_acting = self._internal_env_wrapper.env.current_player.seat_id\n",
    "        range_idx = self._internal_env_wrapper.env.get_range_idx(p_id=p_id_acting)\n",
    "        legal_actions_list = self._internal_env_wrapper.env.get_legal_actions()\n",
    "        a_probs_all_hands = None\n",
    "        \n",
    "        if self.mode == \"BR\":\n",
    "            action = None\n",
    "            best_ev = -1e10 #really bad\n",
    "            \n",
    "            for idx, potential_action in enumerate(node.allowed_actions):\n",
    "                if node.children[idx].ev[p_id_acting,range_idx] > best_ev:\n",
    "                    action = potential_action # deterministic\n",
    "                    best_ev = node.children[idx].ev[p_id_acting,range_idx]\n",
    "            \n",
    "        elif self.mode == \"EVAL\":\n",
    "            a_probs = node.strategy[range_idx,:]\n",
    "            # print(node.strategy, node.strategy.shape)\n",
    "            # print(node.allowed_actions)\n",
    "            # print(\"allowed:\", legal_actions_list)\n",
    "            # print(node.ev, node.ev_br)\n",
    "            action = np.random.choice(node.allowed_actions, p=a_probs)\n",
    "\n",
    "        if step_env:\n",
    "            self._internal_env_wrapper.step(action=action)\n",
    "        \n",
    "        assert(a_probs_all_hands is None)\n",
    "        \n",
    "        return action, a_probs_all_hands\n",
    "        \n",
    "    def get_mode(self):\n",
    "        return \"BESTRESPONSE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from H2HEvaluator import H2HEval\n",
    "\n",
    "agent_file1 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/120/eval_agentAVRG_NET.pkl\"\n",
    "agent_file2 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_2/2/eval_agentAVRG_NET.pkl\"\n",
    "\n",
    "student_agent = EvalAgentDeepRange(t_prof, mode=None, device=None)\n",
    "enemy_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file2)\n",
    "init_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file1)\n",
    "\n",
    "copycat = EvalAgentTree(t_prof, br_agent=enemy_agent, mode=None, device=None)\n",
    "bestresponse = EvalAgentTree(t_prof, br_agent=enemy_agent, mode=None, device=None)\n",
    "bestresponse.mode = \"BR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Played 20000 hands of poker.\n",
      "Player  AVRG_NET: 53.45000076293945 +/- 68.41536406978825\n",
      "Player  BESTRESPONSE: -53.45000076293945 +/- 68.41536406978825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53.45000076293945, 1218.4147336530805)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2HEval(enemy_agent, copycat).h2h_eval(n_games=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Played 20000 hands of poker.\n",
      "Player  AVRG_NET: -2147.5 +/- 74.02282180066341\n",
      "Player  BESTRESPONSE: 2147.5 +/- 74.02282180066341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-2147.5, 1426.3270895805763)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H2HEval(enemy_agent, bestresponse).h2h_eval(n_games=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stack_size = t_prof.eval_stack_sizes[0]\n",
    "\n",
    "gt = PublicTree(env_bldr=rl_util.get_env_builder(t_prof=t_prof),\n",
    "           stack_size=stack_size,\n",
    "           stop_at_street=None,\n",
    "           put_out_new_round_after_limit=True,\n",
    "           is_debugging=t_prof.DEBUGGING)\n",
    "gt.build_tree()\n",
    "print(\"Tree with stack size\", gt.stack_size, \"has\", gt.n_nodes, \"nodes out of which\", gt.n_nonterm,\n",
    "      \"are non-terminal.\")\n",
    "gt.fill_with_agent_policy(agent=init_agent)\n",
    "gt.compute_ev()\n",
    "\n",
    "init_agent._internal_env_wrapper.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.root, gt.root.strategy, gt.root.children[0].strategy, gt.root.ev, gt.root.exploitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.root.env_state['a_seq'] == init_agent._internal_env_wrapper.state_dict()['base']['env']['a_seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_agent._internal_env_wrapper.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.root.children[0].env_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_agent._internal_env_wrapper.env.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.root.children[0].children[0].children[0].action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.root.children[0].action, gt.root.children[1].action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.root.children[0].ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.root.children[0].children[0].children[0].env_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.root.children[0].children[0].children[1].p_id_acting_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
