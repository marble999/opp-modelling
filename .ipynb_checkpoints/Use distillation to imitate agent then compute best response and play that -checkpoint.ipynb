{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ************************** Initing args for:  DEEPCOPY_v0   **************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Idea:\n",
    "\n",
    "1. Use distillation on opponent agent to turn large strategy into smaller space (this has to be done in a Bayesian way?)\n",
    "2. Compute best response to Bayesian distribution\n",
    "3. Run this online?\n",
    "\n",
    "Why does this scale better?\n",
    "\n",
    "1. Maintaining the Bayesian distribution is impossibly expensive\n",
    "    -- we have to use a point estimate but still do it in a somewhat Bayesian way\n",
    "2. \n",
    "3. \n",
    "4.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from RangeAgent import EvalAgentDeepRange\n",
    "from TreeAgent import EvalAgentTree\n",
    "\n",
    "from PokerRL.game.games import StandardLeduc  # or any other game\n",
    "from PokerRL.eval.rl_br.RLBRArgs import RLBRArgs\n",
    "\n",
    "from DeepCFR.EvalAgentDeepCFR import EvalAgentDeepCFR\n",
    "from DeepCFR.TrainingProfile import TrainingProfile\n",
    "from DeepCFR.workers.driver.Driver import Driver\n",
    "\n",
    "rlbr_args = RLBRArgs(\n",
    "    rlbr_bet_set = None\n",
    ")\n",
    "\n",
    "t_prof = TrainingProfile(\n",
    "    name=\"DEEPCOPY_v0\",\n",
    "    nn_type=\"feedforward\",\n",
    "    \n",
    "    max_buffer_size_adv=3e6,\n",
    "    eval_agent_export_freq=20,  # export API to play against the agent\n",
    "    n_traversals_per_iter=1500,\n",
    "    n_batches_adv_training=750,\n",
    "    n_batches_avrg_training=2000,\n",
    "    n_merge_and_table_layer_units_adv=64,\n",
    "    n_merge_and_table_layer_units_avrg=64,\n",
    "    n_units_final_adv=64,\n",
    "    n_units_final_avrg=64,\n",
    "    mini_batch_size_adv=2048,\n",
    "    mini_batch_size_avrg=2048,\n",
    "    init_adv_model=\"last\",\n",
    "    init_avrg_model=\"last\",\n",
    "    use_pre_layers_adv=False,\n",
    "    use_pre_layers_avrg=False,\n",
    "\n",
    "    game_cls=StandardLeduc,\n",
    "\n",
    "    # You can specify one or both modes. Choosing both is useful to compare them.\n",
    "    eval_modes_of_algo=(\n",
    "     # EvalAgentDeepCFR.EVAL_MODE_SINGLE,  # SD-CFR\n",
    "     EvalAgentDeepCFR.EVAL_MODE_AVRG_NET,  # Deep-CFR\n",
    "    ),\n",
    "\n",
    "    DISTRIBUTED=False,\n",
    "    rl_br_args=rlbr_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "action_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def hole_card_onehot(hole_card):\n",
    "    rank = hole_card[0][0]\n",
    "    suit = hole_card[0][1]\n",
    "    out = rank + suit * 3 ## arbitrary but it will learn the relationship\n",
    "    return torch.LongTensor([out])\n",
    "\n",
    "def best_response(agent):\n",
    "    \"\"\"\n",
    "    Returns strategy that is best response to agent strategy\n",
    "    \"\"\"\n",
    "    br = EvalAgentTree(t_prof, br_agent=agent, mode=None, device=None)\n",
    "    br.mode = \"BR\"\n",
    "    return br\n",
    "\n",
    "def train_while_play(student_agent, enemy_agent, args={'lr':1e-2, 'iters':10000}):\n",
    "    \"\"\"\n",
    "    Train student_agent to play against stationery enemy_agent.\n",
    "    \"\"\"\n",
    "            \n",
    "    env_bldr = student_agent.env_bldr\n",
    "    env_cls = env_bldr.env_cls\n",
    "    env_args = env_bldr.env_args\n",
    "    lut_holder = env_cls.get_lut_holder()\n",
    "    \n",
    "    assert(student_agent.env_bldr.env_cls == enemy_agent.env_bldr.env_cls)\n",
    "    assert(env_args.n_seats == 2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(student_agent.policy[0]._net.parameters()) + \\\n",
    "                                 list(student_agent.policy[1]._net.parameters()), lr=args['lr'])\n",
    "    start_time = time.time()\n",
    "    \n",
    "    REFERENCE_AGENT = 0\n",
    "    \n",
    "    _env = env_cls(env_args=env_args, lut_holder=lut_holder, is_evaluating=True)\n",
    "    _eval_agents = [enemy_agent, deepcopy(student_agent)] # play against an new frozen copy of the BR to agent while training\n",
    "    \n",
    "    results = {\n",
    "        \"action_loss\": [],\n",
    "        \"winnings\": []\n",
    "    }\n",
    "    iters = 0 # number of hands played\n",
    "    evals = 0 # number of teaching moments\n",
    "    \n",
    "    # zero grads, set net to train mode\n",
    "    student_agent.policy[0]._net.train()\n",
    "    student_agent.policy[1]._net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    while iters < args['iters']:\n",
    "        iters += 1\n",
    "        \n",
    "        if iters % 200 == 0:\n",
    "            print(\"Iters {} | Evals {} | ActionLoss {} | Winnings mBB/Hand {} | \".format(\n",
    "                iters, evals, sum(results['action_loss']) / evals, sum(results[\"winnings\"]) / iters\n",
    "            ))\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            ## _eval_agents[0] = deepcopy(student_agent)\n",
    "            \n",
    "            # play against an new frozen copy of the BR to agent while training\n",
    "            _eval_agents[1] = best_response(deepcopy(student_agent)) \n",
    "        \n",
    "        for seat_p0 in range(_env.N_SEATS):\n",
    "            seat_p1 = 1 - seat_p0\n",
    "            \n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Reset Episode\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            _, r_for_all, done, info = _env.reset()\n",
    "            for e in _eval_agents:\n",
    "                e.reset(deck_state_dict=_env.cards_state_dict())\n",
    "\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Play Episode\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "            while not done:\n",
    "                p_id_acting = _env.current_player.seat_id\n",
    "\n",
    "                if p_id_acting == seat_p0:\n",
    "                    evals += 1 #increment counter\n",
    "                    \n",
    "                    # set student to position of agent 1, estimate range + actions\n",
    "                    student_agent.set_env_wrapper(_eval_agents[REFERENCE_AGENT]._internal_env_wrapper) \n",
    "                    student_a_probs = student_agent.get_a_probs_tensor()\n",
    "                    \n",
    "                    # get true values \n",
    "                    action_int, _ = _eval_agents[REFERENCE_AGENT].get_action(step_env=True, need_probs=False)\n",
    "                    \n",
    "                    # print(\"True:\", a_probs, range_label)\n",
    "                    # print(\"Prediction:\", student_a_probs, student_range_probs)\n",
    "                    # print(\"Checking requires_grad:\", student_a_probs.requires_grad, student_range_probs.requires_grad)\n",
    "                    \n",
    "                    # compute loss\n",
    "                    loss = action_loss(student_a_probs.view(1,-1), torch.LongTensor([action_int]))\n",
    "                    results['action_loss'].append(loss)\n",
    "                    \n",
    "                    # print(\"Loss:\", rloss, aloss, loss)\n",
    "                    \n",
    "                    # backpropogate\n",
    "                    loss.backward() # accumulate gradients over many steps\n",
    "                                        \n",
    "                    # notify opponent\n",
    "                    _eval_agents[1 - REFERENCE_AGENT].notify_of_action(p_id_acted=p_id_acting,\n",
    "                                                                       action_he_did=action_int)\n",
    "                elif p_id_acting == seat_p1:\n",
    "                    action_int, _ = _eval_agents[1 - REFERENCE_AGENT].get_action(step_env=True,\n",
    "                                                                                 need_probs=False)\n",
    "                    _eval_agents[REFERENCE_AGENT].notify_of_action(p_id_acted=p_id_acting,\n",
    "                                                                   action_he_did=action_int)\n",
    "                else:\n",
    "                    raise ValueError(\"Only HU supported!\")\n",
    "\n",
    "                _, r_for_all, done, info = _env.step(action_int)\n",
    "                \n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Add Rews\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            results[\"winnings\"].append(r_for_all[seat_p0] * _env.REWARD_SCALAR * _env.EV_NORMALIZER)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Time taken\", end_time - start_time)\n",
    "\n",
    "    print(optimizer)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters 200 | Evals 827 | ActionLoss 0.8516446352005005 | Winnings mBB/Hand 145.0 | \n",
      "Iters 400 | Evals 1650 | ActionLoss 0.8496192693710327 | Winnings mBB/Hand 107.5 | \n",
      "Time taken 7.435668230056763\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.02\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent_file1 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/120/eval_agentAVRG_NET.pkl\"\n",
    "agent_file2 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/20/eval_agentAVRG_NET.pkl\"\n",
    "\n",
    "student_agent = EvalAgentDeepRange(t_prof, mode=None, device=None)\n",
    "enemy_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file2)\n",
    "init_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file1)\n",
    "student_agent.policy[0]._net.load_state_dict(init_agent.avrg_net_policies[0]._net.state_dict())\n",
    "student_agent.policy[1]._net.load_state_dict(init_agent.avrg_net_policies[1]._net.state_dict())\n",
    "\n",
    "results = train_while_play(student_agent, enemy_agent, args={'lr':2e-2, 'iters':500})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f52139066d8>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot([sum(results[\"winnings\"][i*200:(i+1)*200])/200 for i in range(100)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random agent vs Best Response\n",
      "\n",
      "Played 200 hands of poker.\n",
      "Player  COPYCAT: -1200.0 +/- 613.7335447963933\n",
      "Player  BESTRESPONSE: 1200.0 +/- 613.7335447963933\n",
      "\n",
      "Random agent vs Nash\n",
      "\n",
      "Played 200 hands of poker.\n",
      "Player  COPYCAT: -520.0 +/- 566.8704910297988\n",
      "Player  AVRG_NET: 520.0 +/- 566.8704910297988\n",
      "\n",
      "Random agent vs Student\n",
      "\n",
      "Played 200 hands of poker.\n",
      "Player  COPYCAT: -535.0 +/- 627.9464256249412\n",
      "Player  BESTRESPONSE: 535.0 +/- 627.9464256249412\n",
      "\n",
      "Weak agent vs Best Response\n",
      "\n",
      "Played 200 hands of poker.\n",
      "Player  AVRG_NET: -55.0 +/- 631.655072252947\n",
      "Player  BESTRESPONSE: 55.0 +/- 631.655072252947\n",
      "\n",
      "Weak agent vs Nash\n",
      "\n",
      "Played 200 hands of poker.\n",
      "Player  AVRG_NET: -25.0 +/- 544.837226719527\n",
      "Player  AVRG_NET: 25.0 +/- 544.837226719527\n",
      "\n",
      "Weak agent vs Student\n",
      "\n",
      "Played 200 hands of poker.\n",
      "Player  AVRG_NET: 335.0 +/- 654.075997497995\n",
      "Player  BESTRESPONSE: -335.0 +/- 654.075997497995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(335.0, 111363.86154284599)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Absolute Best Response\n",
    "\n",
    "from H2HEvaluator import H2HEval\n",
    "\n",
    "agent_file1 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/120/eval_agentAVRG_NET.pkl\"\n",
    "agent_file2 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/20/eval_agentAVRG_NET.pkl\"\n",
    "\n",
    "nash_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file1)\n",
    "weak_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file2)\n",
    "\n",
    "random_agent = EvalAgentTree(t_prof, br_agent=nash_agent, mode=None, device=None)\n",
    "random_agent.tree.fill_uniform_random()\n",
    "random_agent.mode = \"EVAL\"\n",
    "\n",
    "bestresponse = EvalAgentTree(t_prof, br_agent=weak_agent, mode=None, device=None)\n",
    "bestresponse.mode = \"BR\"\n",
    "\n",
    "br_student = EvalAgentTree(t_prof, br_agent=student_agent, mode=None, device=None)\n",
    "br_student.mode = \"BR\"\n",
    "\n",
    "n_games = 100\n",
    "\n",
    "print(\"\\nRandom agent vs Best Response\")\n",
    "H2HEval(random_agent, bestresponse).h2h_eval(n_games=n_games)\n",
    "\n",
    "print(\"\\nRandom agent vs Nash\")\n",
    "H2HEval(random_agent, nash_agent).h2h_eval(n_games=n_games)\n",
    "\n",
    "print(\"\\nRandom agent vs Student\")\n",
    "H2HEval(random_agent, br_student).h2h_eval(n_games=n_games)\n",
    "\n",
    "print(\"\\nWeak agent vs Best Response\")\n",
    "H2HEval(weak_agent, bestresponse).h2h_eval(n_games=n_games)\n",
    "\n",
    "print(\"\\nWeak agent vs Nash\")\n",
    "H2HEval(weak_agent, nash_agent).h2h_eval(n_games=n_games)\n",
    "\n",
    "print(\"\\nWeak agent vs Student\")\n",
    "H2HEval(weak_agent, br_student).h2h_eval(n_games=n_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deep Best Response code below. Works but is really really slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom PokerRL.rl.base_cls.workers.ChiefBase import ChiefBase\\nfrom PokerRL.eval.rl_br.LocalRLBRMaster import LocalRLBRMaster\\nfrom PokerRL.eval.rl_br.workers.ps.Local_RLBR_ParameterServer import Local_RLBR_ParameterServer\\nfrom PokerRL.eval.rl_br.workers.la.Local_RLBR_LearnerActor import Local_RLBR_LearnerActor\\n\\neval_agent_cls = EvalAgentDeepCFR\\ninit_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file1)\\n# init_agent.set_mode('AVRG_NET')\\nchief_handle = ChiefBase(t_prof)\\nlas = [Local_RLBR_LearnerActor(t_prof, chief_handle, eval_agent_cls) for i in range(2)]\\nps = Local_RLBR_ParameterServer(t_prof, chief_handle)\\n\\nmaster = LocalRLBRMaster(t_prof, chief_handle, eval_agent_cls)\\nmaster._eval_agent = init_agent\\nmaster._la_handles = las\\nmaster._ps_handle = ps\\n\\nmaster.evaluate(1)\\n\\nmaster._ps_handle.get_eval_ddqn_state_dicts()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from PokerRL.rl.base_cls.workers.ChiefBase import ChiefBase\n",
    "from PokerRL.eval.rl_br.LocalRLBRMaster import LocalRLBRMaster\n",
    "from PokerRL.eval.rl_br.workers.ps.Local_RLBR_ParameterServer import Local_RLBR_ParameterServer\n",
    "from PokerRL.eval.rl_br.workers.la.Local_RLBR_LearnerActor import Local_RLBR_LearnerActor\n",
    "\n",
    "eval_agent_cls = EvalAgentDeepCFR\n",
    "init_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file1)\n",
    "# init_agent.set_mode('AVRG_NET')\n",
    "chief_handle = ChiefBase(t_prof)\n",
    "las = [Local_RLBR_LearnerActor(t_prof, chief_handle, eval_agent_cls) for i in range(2)]\n",
    "ps = Local_RLBR_ParameterServer(t_prof, chief_handle)\n",
    "\n",
    "master = LocalRLBRMaster(t_prof, chief_handle, eval_agent_cls)\n",
    "master._eval_agent = init_agent\n",
    "master._la_handles = las\n",
    "master._ps_handle = ps\n",
    "\n",
    "master.evaluate(1)\n",
    "\n",
    "master._ps_handle.get_eval_ddqn_state_dicts()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
