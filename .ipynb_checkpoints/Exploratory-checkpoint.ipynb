{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PokerRL.cfr.VanillaCFR import VanillaCFR\n",
    "from PokerRL.game import bet_sets\n",
    "from PokerRL.game.games import DiscretizedNLLeduc\n",
    "from PokerRL.rl.base_cls.workers.ChiefBase import ChiefBase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with stack size [20000, 20000] has 1095 nodes out of which 428 are non-terminal.\n",
      "Iteration:  0\n",
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n",
      "Iteration:  10\n",
      "Iteration:  11\n",
      "Iteration:  12\n",
      "Iteration:  13\n",
      "Iteration:  14\n",
      "Iteration:  15\n",
      "Iteration:  16\n",
      "Iteration:  17\n",
      "Iteration:  18\n",
      "Iteration:  19\n",
      "Iteration:  20\n",
      "Iteration:  21\n",
      "Iteration:  22\n",
      "Iteration:  23\n",
      "Iteration:  24\n",
      "Iteration:  25\n",
      "Iteration:  26\n",
      "Iteration:  27\n",
      "Iteration:  28\n",
      "Iteration:  29\n",
      "Iteration:  30\n",
      "Iteration:  31\n",
      "Iteration:  32\n",
      "Iteration:  33\n",
      "Iteration:  34\n",
      "Iteration:  35\n",
      "Iteration:  36\n",
      "Iteration:  37\n",
      "Iteration:  38\n",
      "Iteration:  39\n",
      "Iteration:  40\n",
      "Iteration:  41\n",
      "Iteration:  42\n",
      "Iteration:  43\n",
      "Iteration:  44\n",
      "Iteration:  45\n",
      "Iteration:  46\n",
      "Iteration:  47\n",
      "Iteration:  48\n",
      "Iteration:  49\n",
      "Iteration:  50\n",
      "Iteration:  51\n",
      "Iteration:  52\n",
      "Iteration:  53\n",
      "Iteration:  54\n",
      "Iteration:  55\n",
      "Iteration:  56\n",
      "Iteration:  57\n",
      "Iteration:  58\n",
      "Iteration:  59\n",
      "Iteration:  60\n",
      "Iteration:  61\n",
      "Iteration:  62\n",
      "Iteration:  63\n",
      "Iteration:  64\n",
      "Iteration:  65\n",
      "Iteration:  66\n",
      "Iteration:  67\n",
      "Iteration:  68\n",
      "Iteration:  69\n",
      "Iteration:  70\n",
      "Iteration:  71\n",
      "Iteration:  72\n",
      "Iteration:  73\n",
      "Iteration:  74\n",
      "Iteration:  75\n",
      "Iteration:  76\n",
      "Iteration:  77\n",
      "Iteration:  78\n",
      "Iteration:  79\n",
      "Iteration:  80\n",
      "Iteration:  81\n",
      "Iteration:  82\n",
      "Iteration:  83\n",
      "Iteration:  84\n",
      "Iteration:  85\n",
      "Iteration:  86\n",
      "Iteration:  87\n",
      "Iteration:  88\n",
      "Iteration:  89\n",
      "Iteration:  90\n",
      "Iteration:  91\n",
      "Iteration:  92\n",
      "Iteration:  93\n",
      "Iteration:  94\n",
      "Iteration:  95\n",
      "Iteration:  96\n",
      "Iteration:  97\n",
      "Iteration:  98\n",
      "Iteration:  99\n",
      "Iteration:  100\n",
      "Iteration:  101\n",
      "Iteration:  102\n",
      "Iteration:  103\n",
      "Iteration:  104\n",
      "Iteration:  105\n",
      "Iteration:  106\n",
      "Iteration:  107\n",
      "Iteration:  108\n",
      "Iteration:  109\n",
      "Iteration:  110\n",
      "Iteration:  111\n",
      "Iteration:  112\n",
      "Iteration:  113\n",
      "Iteration:  114\n",
      "Iteration:  115\n",
      "Iteration:  116\n",
      "Iteration:  117\n",
      "Iteration:  118\n",
      "Iteration:  119\n",
      "Iteration:  120\n",
      "Iteration:  121\n",
      "Iteration:  122\n",
      "Iteration:  123\n",
      "Iteration:  124\n",
      "Iteration:  125\n",
      "Iteration:  126\n",
      "Iteration:  127\n",
      "Iteration:  128\n",
      "Iteration:  129\n",
      "Iteration:  130\n",
      "Iteration:  131\n",
      "Iteration:  132\n",
      "Iteration:  133\n",
      "Iteration:  134\n",
      "Iteration:  135\n",
      "Iteration:  136\n",
      "Iteration:  137\n",
      "Iteration:  138\n",
      "Iteration:  139\n",
      "Iteration:  140\n",
      "Iteration:  141\n",
      "Iteration:  142\n",
      "Iteration:  143\n",
      "Iteration:  144\n",
      "Iteration:  145\n",
      "Iteration:  146\n",
      "Iteration:  147\n",
      "Iteration:  148\n",
      "Iteration:  149\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 150\n",
    "name = \"CFR_EXAMPLE\"\n",
    "\n",
    "# Passing None for t_prof will is enough for ChiefBase. We only use it to log; This CFR impl is not distributed.\n",
    "chief = ChiefBase(t_prof=None)\n",
    "cfr = VanillaCFR(name=name,\n",
    "                 game_cls=DiscretizedNLLeduc,\n",
    "                 agent_bet_set=bet_sets.POT_ONLY,\n",
    "                 chief_handle=chief)\n",
    "s\n",
    "for iter_id in range(n_iterations):\n",
    "    print(\"Iteration: \", iter_id)\n",
    "    cfr.iteration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PokerRL.cfr.VanillaCFR.VanillaCFR at 0x7f6125dda748>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "from PokerRL.rl import rl_util\n",
    "from PokerRL.rl.neural.AvrgStrategyNet import AvrgStrategyNet\n",
    "from PokerRL.rl.neural.NetWrapperBase import NetWrapperArgsBase as _NetWrapperArgsBase\n",
    "from PokerRL.rl.neural.NetWrapperBase import NetWrapperBase as _NetWrapperBase\n",
    "\n",
    "\n",
    "class AvgWrapper(_NetWrapperBase):\n",
    "\n",
    "    def __init__(self, owner, env_bldr, avg_training_args):\n",
    "        super().__init__(\n",
    "            net=AvrgStrategyNet(avrg_net_args=avg_training_args.avg_net_args, env_bldr=env_bldr,\n",
    "                                device=avg_training_args.device_training),\n",
    "            env_bldr=env_bldr,\n",
    "            args=avg_training_args,\n",
    "            owner=owner,\n",
    "            device=avg_training_args.device_training,\n",
    "        )\n",
    "        self._all_range_idxs = torch.arange(self._env_bldr.rules.RANGE_SIZE, device=self.device, dtype=torch.long)\n",
    "\n",
    "    def get_a_probs(self, pub_obses, range_idxs, legal_actions_lists):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pub_obses (list):             list of np arrays of shape [np.arr([history_len, n_features]), ...)\n",
    "            range_idxs (np.ndarray):    array of range_idxs (one for each pub_obs) tensor([2, 421, 58, 912, ...])\n",
    "            legal_actions_lists (list:  list of lists. each 2nd level lists contains ints representing legal actions\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            masks = rl_util.batch_get_legal_action_mask_torch(n_actions=self._env_bldr.N_ACTIONS,\n",
    "                                                              legal_actions_lists=legal_actions_lists,\n",
    "                                                              device=self.device)\n",
    "\n",
    "            return self.get_a_probs2(pub_obses=pub_obses, range_idxs=range_idxs, legal_action_masks=masks)\n",
    "\n",
    "    def get_a_probs2(self, pub_obses, range_idxs, legal_action_masks):\n",
    "        with torch.no_grad():\n",
    "            pred = self._net(pub_obses=pub_obses,\n",
    "                             range_idxs=torch.from_numpy(range_idxs).to(dtype=torch.long, device=self.device),\n",
    "                             legal_action_masks=legal_action_masks)\n",
    "\n",
    "            return nnf.softmax(pred, dim=-1).cpu().numpy()\n",
    "\n",
    "    def get_a_probs_for_each_hand(self, pub_obs, legal_actions_list):\n",
    "        assert isinstance(legal_actions_list[0], int), \"all hands can do the same actions. no need to batch\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mask = rl_util.get_legal_action_mask_torch(n_actions=self._env_bldr.N_ACTIONS,\n",
    "                                                       legal_actions_list=legal_actions_list,\n",
    "                                                       device=self.device, dtype=torch.uint8)\n",
    "            mask = mask.unsqueeze(0).expand(self._env_bldr.rules.RANGE_SIZE, -1)\n",
    "\n",
    "            pred = self._net(pub_obses=[pub_obs] * self._env_bldr.rules.RANGE_SIZE,\n",
    "                             range_idxs=self._all_range_idxs,\n",
    "                             legal_action_masks=mask)\n",
    "\n",
    "            return nnf.softmax(pred, dim=1).cpu().numpy()\n",
    "\n",
    "    def _mini_batch_loop(self, buffer, grad_mngr):\n",
    "        batch_pub_obs_t, \\\n",
    "        batch_a_t, \\\n",
    "        batch_range_idx, \\\n",
    "        batch_legal_action_mask_t \\\n",
    "            = buffer.sample(device=self.device, batch_size=self._args.batch_size)\n",
    "\n",
    "        # [batch_size, n_actions]\n",
    "        pred = self._net(pub_obses=batch_pub_obs_t,\n",
    "                         range_idxs=batch_range_idx,\n",
    "                         legal_action_masks=batch_legal_action_mask_t)\n",
    "\n",
    "        grad_mngr.backprop(pred=pred, target=batch_a_t)\n",
    "\n",
    "\n",
    "class AvgWrapperArgs(_NetWrapperArgsBase):\n",
    "\n",
    "    def __init__(self,\n",
    "                 avg_net_args,\n",
    "                 res_buf_size=1e6,\n",
    "                 min_prob_add_res_buf=0.0,\n",
    "                 batch_size=512,\n",
    "                 n_mini_batches_per_update=1,\n",
    "                 loss_str=\"ce\",\n",
    "                 optim_str=\"rms\",\n",
    "                 lr=0.0002,\n",
    "                 device_training=\"cpu\",\n",
    "                 grad_norm_clipping=10.0,\n",
    "                 ):\n",
    "        super().__init__(batch_size=batch_size,\n",
    "                         n_mini_batches_per_update=n_mini_batches_per_update,\n",
    "                         optim_str=optim_str,\n",
    "                         loss_str=loss_str,\n",
    "                         lr=lr,\n",
    "                         grad_norm_clipping=grad_norm_clipping,\n",
    "                         device_training=device_training)\n",
    "        self.avg_net_args = avg_net_args\n",
    "        self.res_buf_size = int(res_buf_size)\n",
    "        self.min_prob_res_buf = min_prob_add_res_buf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2019 Eric Steinberger\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PokerRL.rl.base_cls.EvalAgentBase import EvalAgentBase as _EvalAgentBase\n",
    "from PokerRL.rl.errors import UnknownModeError\n",
    "\n",
    "\n",
    "class EvalAgentNFSP(_EvalAgentBase):\n",
    "    EVAL_MODE_AVG = \"NFSP_Avg\"\n",
    "    ALL_MODES = [EVAL_MODE_AVG]\n",
    "\n",
    "    def __init__(self, t_prof, mode=None, device=None):\n",
    "        super().__init__(t_prof=t_prof, mode=mode, device=device)\n",
    "        self.avg_args = t_prof.module_args[\"avg\"]\n",
    "\n",
    "        self.policies = [\n",
    "            AvgWrapper(owner=p, env_bldr=self.env_bldr, avg_training_args=self.avg_args)\n",
    "            for p in range(t_prof.n_seats)\n",
    "        ]\n",
    "        for pol in self.policies:\n",
    "            pol.eval()\n",
    "\n",
    "    def can_compute_mode(self):\n",
    "        return True\n",
    "\n",
    "    def get_a_probs_for_each_hand(self):\n",
    "        \"\"\" BEFORE CALLING, NOTIFY EVALAGENT OF THE PAST ACTIONS / ACTIONSEQUENCE!!!!! \"\"\"\n",
    "        p_id_acting = self._internal_env_wrapper.env.current_player.seat_id\n",
    "\n",
    "        if self._mode == self.EVAL_MODE_AVG:\n",
    "            return self.policies[p_id_acting].get_a_probs_for_each_hand(\n",
    "                pub_obs=self._internal_env_wrapper.get_current_obs(),\n",
    "                legal_actions_list=self._internal_env_wrapper.env.get_legal_actions())\n",
    "\n",
    "        else:\n",
    "            raise UnknownModeError(self._mode)\n",
    "\n",
    "    def get_a_probs(self):\n",
    "        p_id_acting = self._internal_env_wrapper.env.current_player.seat_id\n",
    "        range_idx = self._internal_env_wrapper.env.get_range_idx(p_id=p_id_acting)\n",
    "        return self.policies[p_id_acting].get_a_probs(\n",
    "            pub_obses=[self._internal_env_wrapper.get_current_obs()],\n",
    "            range_idxs=np.array([range_idx], dtype=np.int32),\n",
    "            legal_actions_lists=[self._internal_env_wrapper.env.get_legal_actions()]\n",
    "        )[0]\n",
    "\n",
    "    def get_action(self, step_env=True, need_probs=False):\n",
    "        \"\"\" !! BEFORE CALLING, NOTIFY EVALAGENT OF THE PAST ACTIONS / ACTIONSEQUENCE !! \"\"\"\n",
    "\n",
    "        p_id_acting = self._internal_env_wrapper.env.current_player.seat_id\n",
    "        range_idx = self._internal_env_wrapper.env.get_range_idx(p_id=p_id_acting)\n",
    "\n",
    "        if self._mode == self.EVAL_MODE_AVG:\n",
    "            if need_probs:  # only do if rly necessary\n",
    "                a_probs_all_hands = self.get_a_probs_for_each_hand()\n",
    "                a_probs = a_probs_all_hands[range_idx]\n",
    "            else:\n",
    "                a_probs_all_hands = None  # not needed\n",
    "                a_probs = self.policies[p_id_acting].get_a_probs(\n",
    "                    pub_obses=[self._internal_env_wrapper.get_current_obs()],\n",
    "                    range_idxs=np.array([range_idx], dtype=np.int32),\n",
    "                    legal_actions_lists=[self._internal_env_wrapper.env.get_legal_actions()]\n",
    "                )[0]\n",
    "\n",
    "            action = np.random.choice(np.arange(self.env_bldr.N_ACTIONS), p=a_probs)\n",
    "\n",
    "            if step_env:\n",
    "                self._internal_env_wrapper.step(action=action)\n",
    "\n",
    "            return action, a_probs_all_hands\n",
    "\n",
    "        else:\n",
    "            raise UnknownModeError(self._mode)\n",
    "\n",
    "    def update_weights(self, weights_for_eval_agent):\n",
    "        for i in range(self.t_prof.n_seats):\n",
    "            self.policies[i].load_net_state_dict(self.ray.state_dict_to_torch(weights_for_eval_agent[i],\n",
    "                                                                              device=self.device))\n",
    "            self.policies[i].eval()\n",
    "\n",
    "    def _state_dict(self):\n",
    "        return {\n",
    "            \"net_state_dicts\": [pol.net_state_dict() for pol in self.policies],\n",
    "        }\n",
    "\n",
    "    def _load_state_dict(self, state_dict):\n",
    "        for i in range(self.t_prof.n_seats):\n",
    "            self.policies[i].load_net_state_dict(state_dict[\"net_state_dicts\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "from PokerRL.game import bet_sets\n",
    "from PokerRL.game.games import StandardLeduc\n",
    "from PokerRL.game.wrappers import HistoryEnvBuilder, FlatLimitPokerEnvBuilder\n",
    "from PokerRL.rl.agent_modules.DDQN import DDQNArgs\n",
    "from PokerRL.rl.base_cls.TrainingProfileBase import TrainingProfileBase\n",
    "from PokerRL.rl.neural.AvrgStrategyNet import AvrgNetArgs\n",
    "from PokerRL.rl.neural.DuelingQNet import DuelingQArgs\n",
    "from PokerRL.rl.neural.MainPokerModuleFLAT import MPMArgsFLAT\n",
    "from PokerRL.rl.neural.MainPokerModuleRNN import MPMArgsRNN\n",
    "\n",
    "class TrainingProfile(TrainingProfileBase):\n",
    "\n",
    "    def __init__(self,\n",
    "\n",
    "                 # --- general\n",
    "                 name,\n",
    "                 log_export_freq=200,\n",
    "                 checkpoint_freq=99999999,\n",
    "                 eval_agent_export_freq=99999999,\n",
    "\n",
    "                 # --- Computing\n",
    "                 path_data=None,\n",
    "                 local_crayon_server_docker_address=\"localhost\",\n",
    "                 device_inference=\"cpu\",\n",
    "                 device_parameter_server=\"cpu\",\n",
    "                 n_learner_actor_workers=8,\n",
    "                 max_n_las_sync_simultaneously=100,\n",
    "                 DISTRIBUTED=False,\n",
    "                 CLUSTER=False,\n",
    "                 DEBUGGING=False,\n",
    "                 VERBOSE=True,\n",
    "\n",
    "                 # --- env\n",
    "                 game_cls=StandardLeduc,\n",
    "                 n_seats=2,\n",
    "                 use_simplified_headsup_obs=True,\n",
    "                 start_chips=None,\n",
    "\n",
    "                 agent_bet_set=bet_sets.B_2,\n",
    "                 stack_randomization_range=(0, 0),\n",
    "                 uniform_action_interpolation=False,\n",
    "\n",
    "                 # --- Evaluation\n",
    "                 eval_modes_of_algo=(EvalAgentNFSP.EVAL_MODE_AVG,),\n",
    "                 eval_stack_sizes=None,\n",
    "\n",
    "                 # --- NFSP\n",
    "                 nn_type=\"feedforward\",\n",
    "                 anticipatory_parameter=0.1,\n",
    "\n",
    "                 # Original NFSP also adds epsilon-exploration actions to the averaging buffer.\n",
    "                 add_random_actions_to_avg_buffer=True,\n",
    "\n",
    "                 n_br_updates_per_iter=2,\n",
    "                 n_avg_updates_per_iter=2,\n",
    "                 target_net_update_freq=300,  # every N neural net updates. Not every N global iters, episodes, or steps\n",
    "                 cir_buf_size_each_la=2e5,\n",
    "                 res_buf_size_each_la=2e6,  # the more the better to infinity\n",
    "                 min_prob_add_res_buf=0.0,  # 0.0 =  vanilla reservoir; >0 exponential averaging.\n",
    "\n",
    "                 eps_start=0.06,\n",
    "                 eps_const=0.01,\n",
    "                 eps_exponent=0.5,\n",
    "                 eps_min=0.0,\n",
    "\n",
    "                 # --- Training.\n",
    "                 n_steps_per_iter_per_la=128,\n",
    "                 n_steps_pretrain_per_la=0,\n",
    "                 n_envs=128,\n",
    "\n",
    "                 mini_batch_size_br_per_la=128,\n",
    "                 n_mini_batches_per_la_per_update_br=1,  # total num of samples per iter is that * batch_size above.\n",
    "                 mini_batch_size_avg_per_la=128,\n",
    "                 n_mini_batches_per_la_per_update_avg=1,  # total num of samples per iter is that * batch_size above.\n",
    "                 training_multiplier_iter_0=1,  # In iter 0 the BR net is clueless, but adds to res_buf. -> \"pretrain\"\n",
    "\n",
    "                 # --- Q-Learning Hyperparameters\n",
    "                 n_cards_state_units_br=192,\n",
    "                 n_merge_and_table_layer_units_br=64,\n",
    "                 n_units_final_br=64,\n",
    "                 normalize_last_layer_flat=False,\n",
    "                 rnn_cls_str_br=\"lstm\",\n",
    "                 rnn_units_br=128,\n",
    "                 rnn_stack_br=1,\n",
    "                 lr_br=0.1,\n",
    "                 dropout_br=0.0,\n",
    "                 use_pre_layers_br=True,  # True -> Use deep multi-branch network; False -> Use shallow net\n",
    "                 grad_norm_clipping_br=10.0,\n",
    "                 optimizer_br=\"sgd\",\n",
    "                 loss_br=\"mse\",\n",
    "\n",
    "                 # --- Avg Network Hyperparameters\n",
    "                 n_cards_state_units_avg=192,\n",
    "                 n_merge_and_table_layer_units_avg=64,\n",
    "                 n_units_final_avg=64,\n",
    "                 rnn_cls_str_avg=\"lstm\",\n",
    "                 rnn_units_avg=128,\n",
    "                 rnn_stack_avg=1,\n",
    "                 lr_avg=0.005,\n",
    "                 dropout_avg=0.0,\n",
    "                 use_pre_layers_avg=True,  # True -> Use deep multi-branch network; False -> Use shallow net\n",
    "                 grad_norm_clipping_avg=10.0,\n",
    "                 optimizer_avg=\"sgd\",\n",
    "                 loss_avg=\"ce\",\n",
    "\n",
    "                 # Option\n",
    "                 lbr_args=None,\n",
    "                 rlbr_args=None,\n",
    "                 ):\n",
    "        print(\" ************************** Initing args for: \", name, \"  **************************\")\n",
    "\n",
    "        if nn_type == \"recurrent\":\n",
    "            env_bldr_cls = HistoryEnvBuilder\n",
    "\n",
    "            mpm_args_br = MPMArgsRNN(rnn_cls_str=rnn_cls_str_br,\n",
    "                                     rnn_units=rnn_units_br,\n",
    "                                     rnn_stack=rnn_stack_br,\n",
    "                                     rnn_dropout=dropout_br,\n",
    "                                     use_pre_layers=use_pre_layers_br,\n",
    "                                     n_cards_state_units=n_cards_state_units_br,\n",
    "                                     n_merge_and_table_layer_units=n_merge_and_table_layer_units_br)\n",
    "            mpm_args_avg = MPMArgsRNN(rnn_cls_str=rnn_cls_str_avg,\n",
    "                                      rnn_units=rnn_units_avg,\n",
    "                                      rnn_stack=rnn_stack_avg,\n",
    "                                      rnn_dropout=dropout_avg,\n",
    "                                      use_pre_layers=use_pre_layers_avg,\n",
    "                                      n_cards_state_units=n_cards_state_units_avg,\n",
    "                                      n_merge_and_table_layer_units=n_merge_and_table_layer_units_avg)\n",
    "\n",
    "        elif nn_type == \"feedforward\":\n",
    "            env_bldr_cls = FlatLimitPokerEnvBuilder\n",
    "\n",
    "            mpm_args_br = MPMArgsFLAT(use_pre_layers=use_pre_layers_br,\n",
    "                                      card_block_units=n_cards_state_units_br,\n",
    "                                      other_units=n_merge_and_table_layer_units_br,\n",
    "                                      normalize=normalize_last_layer_flat,\n",
    "                                      )\n",
    "            mpm_args_avg = MPMArgsFLAT(use_pre_layers=use_pre_layers_avg,\n",
    "                                       card_block_units=n_cards_state_units_avg,\n",
    "                                       other_units=n_merge_and_table_layer_units_avg)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(nn_type)\n",
    "\n",
    "        super().__init__(\n",
    "\n",
    "            name=name,\n",
    "            log_verbose=VERBOSE,\n",
    "            log_export_freq=log_export_freq,\n",
    "            checkpoint_freq=checkpoint_freq,\n",
    "            eval_agent_export_freq=eval_agent_export_freq,\n",
    "            path_data=path_data,\n",
    "            game_cls=game_cls,\n",
    "            env_bldr_cls=env_bldr_cls,\n",
    "            start_chips=start_chips,\n",
    "            eval_modes_of_algo=eval_modes_of_algo,\n",
    "            eval_stack_sizes=eval_stack_sizes,\n",
    "\n",
    "            DEBUGGING=DEBUGGING,\n",
    "            DISTRIBUTED=DISTRIBUTED,\n",
    "            CLUSTER=CLUSTER,\n",
    "            device_inference=device_inference,\n",
    "            local_crayon_server_docker_address=local_crayon_server_docker_address,\n",
    "\n",
    "            module_args={\n",
    "                \"ddqn\": DDQNArgs(\n",
    "                    q_args=DuelingQArgs(\n",
    "                        mpm_args=mpm_args_br,\n",
    "                        n_units_final=n_units_final_br,\n",
    "                    ),\n",
    "                    cir_buf_size=int(cir_buf_size_each_la),\n",
    "                    batch_size=mini_batch_size_br_per_la,\n",
    "                    n_mini_batches_per_update=n_mini_batches_per_la_per_update_br,\n",
    "                    target_net_update_freq=target_net_update_freq,\n",
    "                    optim_str=optimizer_br,\n",
    "                    loss_str=loss_br,\n",
    "                    lr=lr_br,\n",
    "                    eps_start=eps_start,\n",
    "                    eps_const=eps_const,\n",
    "                    eps_exponent=eps_exponent,\n",
    "                    eps_min=eps_min,\n",
    "                    grad_norm_clipping=grad_norm_clipping_br,\n",
    "                ),\n",
    "                \"avg\": AvgWrapperArgs(\n",
    "                    avg_net_args=AvrgNetArgs(\n",
    "                        mpm_args=mpm_args_avg,\n",
    "                        n_units_final=n_units_final_avg,\n",
    "                    ),\n",
    "                    batch_size=mini_batch_size_avg_per_la,\n",
    "                    n_mini_batches_per_update=n_mini_batches_per_la_per_update_avg,\n",
    "                    res_buf_size=int(res_buf_size_each_la),\n",
    "                    min_prob_add_res_buf=min_prob_add_res_buf,\n",
    "                    loss_str=loss_avg,\n",
    "                    optim_str=optimizer_avg,\n",
    "                    lr=lr_avg,\n",
    "                    grad_norm_clipping=grad_norm_clipping_avg,\n",
    "                ),\n",
    "                \"env\": game_cls.ARGS_CLS(\n",
    "                    n_seats=n_seats,\n",
    "                    starting_stack_sizes_list=[start_chips for _ in range(n_seats)],\n",
    "                    stack_randomization_range=stack_randomization_range,\n",
    "                    use_simplified_headsup_obs=use_simplified_headsup_obs,\n",
    "                    uniform_action_interpolation=uniform_action_interpolation,\n",
    "\n",
    "                    # Set up in a way that just ignores this if not Discretized\n",
    "                    bet_sizes_list_as_frac_of_pot=copy.deepcopy(agent_bet_set),\n",
    "                ),\n",
    "                \"lbr\": lbr_args,\n",
    "                \"rlbr\": rlbr_args,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # ____________________________________________________ NFSP ____________________________________________________\n",
    "        self.nn_type = nn_type\n",
    "        self.n_br_updates_per_iter = int(n_br_updates_per_iter)\n",
    "        self.n_avg_updates_per_iter = int(n_avg_updates_per_iter)\n",
    "        self.anticipatory_parameter = anticipatory_parameter\n",
    "        self.add_random_actions_to_buffer = add_random_actions_to_avg_buffer\n",
    "        self.training_multiplier_iter_0 = int(training_multiplier_iter_0)\n",
    "        self.n_envs = int(n_envs)\n",
    "        self.n_steps_pretrain_per_la = int(n_steps_pretrain_per_la)\n",
    "        self.n_steps_per_iter_per_la = int(n_steps_per_iter_per_la)\n",
    "\n",
    "        if DISTRIBUTED or CLUSTER:\n",
    "            self.n_learner_actors = int(n_learner_actor_workers)\n",
    "        else:\n",
    "            self.n_learner_actors = 1\n",
    "\n",
    "        self.max_n_las_sync_simultaneously = int(max_n_las_sync_simultaneously)\n",
    "\n",
    "        assert isinstance(device_parameter_server, str), \"Please pass a string (either 'cpu' or 'cuda')!\"\n",
    "        self.device_parameter_server = torch.device(device_parameter_server)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Play an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ************************** Initing args for:  test   **************************\n"
     ]
    }
   ],
   "source": [
    "t_prof = TrainingProfile(name='test')\n",
    "eval_agent = EvalAgentNFSP(t_prof=t_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       \n",
      "                                                _____\n",
      "                    _____                _____ |6    |\n",
      "                   |2    | _____        |5    || & & | \n",
      "                   |  &  ||3    | _____ | & & || & & | _____\n",
      "                   |     || & & ||4    ||  &  || & & ||7    |\n",
      "                   |  &  ||     || & & || & & ||____9|| & & | _____\n",
      "                   |____Z||  &  ||     ||____S|       |& & &||8    | _____\n",
      "                          |____E|| & & |              | & & ||& & &||9    |\n",
      "                                 |____h|              |____L|| & & ||& & &|\n",
      "                                                             |& & &||& & &|\n",
      "                                                             |____8||& & &|\n",
      "                                                                    |____6|\n",
      "               \n",
      "____________________________________________ TUTORIAL ____________________________________________\n",
      "Actions:\n",
      "0 \tFold\n",
      "1 \tCall\n",
      "2 \tRaise according to current fixed limit\n",
      "\n",
      "****************************\n",
      "*        GAME START        *\n",
      "****************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "___________________________________ preflop - 0 acts ___________________________________\n",
      "Board:  \n",
      "Last Action:   player_None: None                                                                                                              None |   Main_pot:        2\n",
      "     Player_0:stack:        12 current_bet:         0 side_pot_rank:        -1 hand:      3b,                     |   Side_pot0:      0\n",
      "     Player_1:stack:        12 current_bet:         0 side_pot_rank:        -1 hand:      2a,                     |   Side_pot1:      0\n",
      "Num raises this round:  0\n",
      "\n",
      "\n",
      "What action do you want to take as player 0?1\n",
      "\n",
      "\n",
      "\n",
      "___________________________________ preflop - 1 acts ___________________________________\n",
      "Board:  \n",
      "Last Action:   player_0: 1                                                                                                                 0 |   Main_pot:        2\n",
      "     Player_0:stack:        12 current_bet:         0 side_pot_rank:        -1 hand:      3b,                     |   Side_pot0:      0\n",
      "     Player_1:stack:        12 current_bet:         0 side_pot_rank:        -1 hand:      2a,                     |   Side_pot1:      0\n",
      "Num raises this round:  0\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f116a0184455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                        )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_to_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/leduc3/lib/python3.7/site-packages/PokerRL/game/InteractiveGame.py\u001b[0m in \u001b[0;36mstart_to_play\u001b[0;34m(self, render_mode, limit_numpy_digits)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;31m# Agent acts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0ma_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action_frac_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ma_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                         action_tuple = [2, self._env.get_fraction_of_pot_raise(fraction=frac,\n",
      "\u001b[0;32m~/miniconda3/envs/leduc3/lib/python3.7/site-packages/PokerRL/rl/base_cls/EvalAgentBase.py\u001b[0m in \u001b[0;36mget_action_frac_tuple\u001b[0;34m(self, step_env)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFOLD\u001b[0m \u001b[0mCALL\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mRAISE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PokerRL.game import InteractiveGame\n",
    "\n",
    "game = InteractiveGame(env_cls=eval_agent.env_bldr.env_cls,\n",
    "                       env_args=eval_agent.env_bldr.env_args,\n",
    "                       seats_human_plays_list=[0],\n",
    "                       eval_agent=eval_agent,\n",
    "                       )\n",
    "\n",
    "game.start_to_play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
