{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RangeAgent import EvalAgentDeepRange\n",
    "from PokerRL.game.games import StandardLeduc  # or any other game\n",
    "\n",
    "from DeepCFR.EvalAgentDeepCFR import EvalAgentDeepCFR\n",
    "from DeepCFR.TrainingProfile import TrainingProfile\n",
    "from DeepCFR.workers.driver.Driver import Driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ************************** Initing args for:  DEEP_RANGE_v0   **************************\n"
     ]
    }
   ],
   "source": [
    "t_prof = TrainingProfile(\n",
    "    name=\"DEEP_RANGE_v0\",\n",
    "    nn_type=\"feedforward\",\n",
    "    \n",
    "    max_buffer_size_adv=3e6,\n",
    "    eval_agent_export_freq=20,  # export API to play against the agent\n",
    "    n_traversals_per_iter=1500,\n",
    "    n_batches_adv_training=750,\n",
    "    n_batches_avrg_training=2000,\n",
    "    n_merge_and_table_layer_units_adv=64,\n",
    "    n_merge_and_table_layer_units_avrg=64,\n",
    "    n_units_final_adv=64,\n",
    "    n_units_final_avrg=64,\n",
    "    mini_batch_size_adv=2048,\n",
    "    mini_batch_size_avrg=2048,\n",
    "    init_adv_model=\"last\",\n",
    "    init_avrg_model=\"last\",\n",
    "    use_pre_layers_adv=False,\n",
    "    use_pre_layers_avrg=False,\n",
    "\n",
    "    game_cls=StandardLeduc,\n",
    "\n",
    "    # You can specify one or both modes. Choosing both is useful to compare them.\n",
    "    eval_modes_of_algo=(\n",
    "     EvalAgentDeepCFR.EVAL_MODE_SINGLE,  # SD-CFR\n",
    "     EvalAgentDeepCFR.EVAL_MODE_AVRG_NET,  # Deep CFR\n",
    "    ),\n",
    "\n",
    "    DISTRIBUTED=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeActionNet(\n",
       "  (range_net): RangeNet(\n",
       "    (_relu): ReLU()\n",
       "    (_mpm): MainPokerModuleFLAT(\n",
       "      (_relu): ReLU()\n",
       "      (final_fc_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (final_fc_2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (_final_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (_out_layer): Linear(in_features=64, out_features=6, bias=True)\n",
       "    (_softmax): Softmax()\n",
       "  )\n",
       "  (_relu): ReLU()\n",
       "  (_final_layer): Linear(in_features=27, out_features=64, bias=True)\n",
       "  (_out_layer): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvalAgentDeepRange(t_prof, mode=None, device=None).policy._net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "range_loss = nn.CrossEntropyLoss()\n",
    "action_loss = nn.MSELoss() ## cross-entropy would be ideal\n",
    "\n",
    "def hole_card_onehot(hole_card):\n",
    "    rank = hole_card[0][0]\n",
    "    suit = hole_card[0][1]\n",
    "    out = rank + suit * 3 ## arbitrary but it will learn the relationship\n",
    "    return torch.LongTensor([out])\n",
    "\n",
    "def distill(student_agent, teacher_agent, args={'lr':1e-2, 'iters':10000, 'lambda':10}):\n",
    "    \"\"\"\n",
    "    Distill student_agent to play like teacher_agent\n",
    "    \"\"\"\n",
    "    \n",
    "    env_bldr = student_agent.env_bldr\n",
    "    env_cls = env_bldr.env_cls\n",
    "    env_args = env_bldr.env_args\n",
    "    lut_holder = env_cls.get_lut_holder()\n",
    "    \n",
    "    assert(student_agent.env_bldr.env_cls == teacher_agent.env_bldr.env_cls)\n",
    "    assert(env_args.n_seats == 2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(student_agent.policy._net.parameters()), lr=args['lr'])\n",
    "    start_time = time.time()\n",
    "    \n",
    "    REFERENCE_AGENT = 0\n",
    "    \n",
    "    _env = env_cls(env_args=env_args, lut_holder=lut_holder, is_evaluating=True)\n",
    "    _eval_agents = [teacher_agent, deepcopy(teacher_agent)]\n",
    "    \n",
    "    results = {\n",
    "        \"range_loss\": [],\n",
    "        \"action_loss\": [],\n",
    "        \"total_loss\": [],\n",
    "    }\n",
    "    iters = 0 # number of hands played\n",
    "    evals = 0 # number of teaching moments\n",
    "    \n",
    "    # zero grads, set net to train mode\n",
    "    student_agent.policy._net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    while iters < args['iters']:\n",
    "        iters += 1\n",
    "        \n",
    "        if iters % 200 == 0:\n",
    "            print(\"Iters {} | Evals {} | RangeLoss {} | ActionLoss {} | TotalLoss {}\".format(\n",
    "                iters, evals, sum(results['range_loss']) / evals, sum(results['action_loss']) / evals, sum(results['total_loss']) / evals\n",
    "            ))\n",
    "            \n",
    "            # print(\"gradient:\", list(student_agent.policy._net.parameters())[0].grad)\n",
    "\n",
    "            # print(\"old params:\", list(student_agent.policy._net.parameters())[0])\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(\"new params:\", list(student_agent.policy._net.parameters())[0])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        for seat_p0 in range(_env.N_SEATS):\n",
    "            seat_p1 = 1 - seat_p0\n",
    "            \n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Reset Episode\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            _, r_for_all, done, info = _env.reset()\n",
    "            for e in _eval_agents + [student_agent]:\n",
    "                e.reset(deck_state_dict=_env.cards_state_dict())\n",
    "\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Play Episode\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "            while not done:\n",
    "                p_id_acting = _env.current_player.seat_id\n",
    "\n",
    "                if p_id_acting == seat_p0:\n",
    "                    evals += 1 #increment counter\n",
    "                    \n",
    "                    # set student to position of agent 1, estimate range + actions\n",
    "                    student_agent.set_env_wrapper(_eval_agents[REFERENCE_AGENT]._internal_env_wrapper) \n",
    "                    student_a_probs = student_agent.get_a_probs_tensor()\n",
    "                    student_range_probs = student_agent.get_range_probs()\n",
    "                    \n",
    "                    # get true values \n",
    "                    a_probs = torch.Tensor(_eval_agents[REFERENCE_AGENT].get_a_probs())\n",
    "                    range_label = _env.get_hole_cards_of_player(seat_p1) #get opponent's true range\n",
    "                    range_label = hole_card_onehot(range_label) # convert to label\n",
    "                    action_int, _ = _eval_agents[REFERENCE_AGENT].get_action(step_env=True, need_probs=False)\n",
    "                    \n",
    "                    # print(\"True:\", a_probs, range_label)\n",
    "                    # print(\"Prediction:\", student_a_probs, student_range_probs)\n",
    "                    # print(\"Checking requires_grad:\", student_a_probs.requires_grad, student_range_probs.requires_grad)\n",
    "                    \n",
    "                    # compute loss\n",
    "                    rloss = range_loss(student_range_probs.view(1,-1), range_label)\n",
    "                    aloss = action_loss(student_a_probs, a_probs)\n",
    "                    loss = rloss + args['lambda'] * aloss\n",
    "                    \n",
    "                    results['total_loss'].append(loss)\n",
    "                    results['range_loss'].append(rloss)\n",
    "                    results['action_loss'].append(aloss)\n",
    "                    \n",
    "                    # print(\"Loss:\", rloss, aloss, loss)\n",
    "                    \n",
    "                    # backpropogate\n",
    "                    loss.backward() # accumulate gradients over many steps\n",
    "                                        \n",
    "                    # notify opponent\n",
    "                    _eval_agents[1 - REFERENCE_AGENT].notify_of_action(p_id_acted=p_id_acting,\n",
    "                                                                       action_he_did=action_int)\n",
    "                elif p_id_acting == seat_p1:\n",
    "                    a_probs = _eval_agents[REFERENCE_AGENT].get_a_probs()\n",
    "                    action_int, _ = _eval_agents[1 - REFERENCE_AGENT].get_action(step_env=True, need_probs=False)\n",
    "                    _eval_agents[REFERENCE_AGENT].notify_of_action(p_id_acted=p_id_acting,\n",
    "                                                                   action_he_did=action_int)\n",
    "                else:\n",
    "                    raise ValueError(\"Only HU supported!\")\n",
    "                \n",
    "                _, r_for_all, done, info = _env.step(action_int)  \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Time taken\", end_time - start_time)\n",
    "\n",
    "    print(optimizer)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters 200 | Evals 814 | RangeLoss 1.7955774068832397 | ActionLoss 0.07010836154222488 | TotalLoss 2.146121025085449\n",
      "Iters 400 | Evals 1647 | RangeLoss 1.7900111675262451 | ActionLoss 0.06836526840925217 | TotalLoss 2.1318373680114746\n",
      "Iters 600 | Evals 2462 | RangeLoss 1.7845299243927002 | ActionLoss 0.06678110361099243 | TotalLoss 2.1184351444244385\n",
      "Iters 800 | Evals 3259 | RangeLoss 1.7796403169631958 | ActionLoss 0.06599391251802444 | TotalLoss 2.1096062660217285\n",
      "Iters 1000 | Evals 4098 | RangeLoss 1.7755833864212036 | ActionLoss 0.06611665338277817 | TotalLoss 2.106161594390869\n",
      "Iters 1200 | Evals 4931 | RangeLoss 1.769439935684204 | ActionLoss 0.06510770320892334 | TotalLoss 2.094968557357788\n",
      "Iters 1400 | Evals 5760 | RangeLoss 1.7679038047790527 | ActionLoss 0.06519202888011932 | TotalLoss 2.0938565731048584\n",
      "Iters 1600 | Evals 6592 | RangeLoss 1.7644996643066406 | ActionLoss 0.06483491510152817 | TotalLoss 2.0886728763580322\n",
      "Iters 1800 | Evals 7448 | RangeLoss 1.7620487213134766 | ActionLoss 0.06479272991418839 | TotalLoss 2.086015462875366\n",
      "Iters 2000 | Evals 8288 | RangeLoss 1.75923752784729 | ActionLoss 0.06434859335422516 | TotalLoss 2.0809805393218994\n",
      "Iters 2200 | Evals 9137 | RangeLoss 1.756768822669983 | ActionLoss 0.06416420638561249 | TotalLoss 2.077587604522705\n",
      "Iters 2400 | Evals 9982 | RangeLoss 1.754456639289856 | ActionLoss 0.06359028071165085 | TotalLoss 2.0724074840545654\n",
      "Iters 2600 | Evals 10804 | RangeLoss 1.754218578338623 | ActionLoss 0.06318109482526779 | TotalLoss 2.070122718811035\n",
      "Iters 2800 | Evals 11624 | RangeLoss 1.753516435623169 | ActionLoss 0.06264970451593399 | TotalLoss 2.066772937774658\n",
      "Iters 3000 | Evals 12477 | RangeLoss 1.752380609512329 | ActionLoss 0.06181833893060684 | TotalLoss 2.061476707458496\n",
      "Iters 3200 | Evals 13314 | RangeLoss 1.7507463693618774 | ActionLoss 0.06114702671766281 | TotalLoss 2.0564818382263184\n",
      "Iters 3400 | Evals 14166 | RangeLoss 1.751024842262268 | ActionLoss 0.06048304960131645 | TotalLoss 2.053443670272827\n",
      "Iters 3600 | Evals 15010 | RangeLoss 1.7506699562072754 | ActionLoss 0.05947483703494072 | TotalLoss 2.048049211502075\n",
      "Iters 3800 | Evals 15839 | RangeLoss 1.7495344877243042 | ActionLoss 0.05862421169877052 | TotalLoss 2.042654037475586\n",
      "Iters 4000 | Evals 16677 | RangeLoss 1.7478276491165161 | ActionLoss 0.05795856937766075 | TotalLoss 2.0376150608062744\n",
      "Iters 4200 | Evals 17535 | RangeLoss 1.7474690675735474 | ActionLoss 0.05709303915500641 | TotalLoss 2.03293514251709\n",
      "Iters 4400 | Evals 18370 | RangeLoss 1.7464295625686646 | ActionLoss 0.05635637417435646 | TotalLoss 2.028214693069458\n",
      "Iters 4600 | Evals 19214 | RangeLoss 1.746670126914978 | ActionLoss 0.055501192808151245 | TotalLoss 2.0241856575012207\n",
      "Iters 4800 | Evals 20044 | RangeLoss 1.7467108964920044 | ActionLoss 0.054629940539598465 | TotalLoss 2.019873857498169\n",
      "Iters 5000 | Evals 20850 | RangeLoss 1.746013879776001 | ActionLoss 0.05390218645334244 | TotalLoss 2.015540361404419\n",
      "Iters 5200 | Evals 21683 | RangeLoss 1.744706392288208 | ActionLoss 0.053131066262722015 | TotalLoss 2.0103812217712402\n",
      "Iters 5400 | Evals 22508 | RangeLoss 1.7444188594818115 | ActionLoss 0.05236531049013138 | TotalLoss 2.0062642097473145\n",
      "Iters 5600 | Evals 23326 | RangeLoss 1.744242787361145 | ActionLoss 0.05156269669532776 | TotalLoss 2.0020790100097656\n",
      "Iters 5800 | Evals 24169 | RangeLoss 1.744506597518921 | ActionLoss 0.050869014114141464 | TotalLoss 1.998878002166748\n",
      "Iters 6000 | Evals 25002 | RangeLoss 1.744138240814209 | ActionLoss 0.05025926232337952 | TotalLoss 1.995458960533142\n",
      "Iters 6200 | Evals 25824 | RangeLoss 1.743780493736267 | ActionLoss 0.04960044100880623 | TotalLoss 1.9918079376220703\n",
      "Iters 6400 | Evals 26653 | RangeLoss 1.7431806325912476 | ActionLoss 0.04888201877474785 | TotalLoss 1.9876173734664917\n",
      "Iters 6600 | Evals 27501 | RangeLoss 1.7434231042861938 | ActionLoss 0.04831898212432861 | TotalLoss 1.9850397109985352\n",
      "Iters 6800 | Evals 28344 | RangeLoss 1.7435472011566162 | ActionLoss 0.04768729954957962 | TotalLoss 1.9819979667663574\n",
      "Iters 7000 | Evals 29172 | RangeLoss 1.7439547777175903 | ActionLoss 0.047100163996219635 | TotalLoss 1.9794679880142212\n",
      "Iters 7200 | Evals 30022 | RangeLoss 1.743545651435852 | ActionLoss 0.04648813232779503 | TotalLoss 1.9759989976882935\n",
      "Iters 7400 | Evals 30860 | RangeLoss 1.7432607412338257 | ActionLoss 0.04583162069320679 | TotalLoss 1.9724304676055908\n",
      "Iters 7600 | Evals 31719 | RangeLoss 1.7430816888809204 | ActionLoss 0.045260049402713776 | TotalLoss 1.9693971872329712\n",
      "Iters 7800 | Evals 32548 | RangeLoss 1.742588996887207 | ActionLoss 0.0446917898952961 | TotalLoss 1.9660735130310059\n",
      "Iters 8000 | Evals 33402 | RangeLoss 1.7424229383468628 | ActionLoss 0.0440438874065876 | TotalLoss 1.962669014930725\n",
      "Iters 8200 | Evals 34237 | RangeLoss 1.74229896068573 | ActionLoss 0.04341985285282135 | TotalLoss 1.95941960811615\n",
      "Iters 8400 | Evals 35080 | RangeLoss 1.7422667741775513 | ActionLoss 0.04282998666167259 | TotalLoss 1.9564332962036133\n",
      "Iters 8600 | Evals 35922 | RangeLoss 1.7423163652420044 | ActionLoss 0.042290885001420975 | TotalLoss 1.9537866115570068\n",
      "Iters 8800 | Evals 36736 | RangeLoss 1.7421718835830688 | ActionLoss 0.04173411801457405 | TotalLoss 1.9508609771728516\n",
      "Iters 9000 | Evals 37571 | RangeLoss 1.7422447204589844 | ActionLoss 0.041189923882484436 | TotalLoss 1.9482064247131348\n",
      "Iters 9200 | Evals 38406 | RangeLoss 1.7421795129776 | ActionLoss 0.04067137837409973 | TotalLoss 1.9455457925796509\n",
      "Iters 9400 | Evals 39264 | RangeLoss 1.742422103881836 | ActionLoss 0.040085241198539734 | TotalLoss 1.9428573846817017\n",
      "Iters 9600 | Evals 40112 | RangeLoss 1.7426687479019165 | ActionLoss 0.03951875865459442 | TotalLoss 1.9402847290039062\n",
      "Iters 9800 | Evals 40988 | RangeLoss 1.7423896789550781 | ActionLoss 0.038921356201171875 | TotalLoss 1.9370107650756836\n",
      "Iters 10000 | Evals 41816 | RangeLoss 1.742594599723816 | ActionLoss 0.03841584920883179 | TotalLoss 1.934696078300476\n",
      "Time taken 284.7581214904785\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Distillation\n",
    "\n",
    "agent_file1 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/120/eval_agentAVRG_NET.pkl\"\n",
    "\n",
    "student_agent = EvalAgentDeepRange(t_prof, mode=None, device=None)\n",
    "teacher_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file1)\n",
    "\n",
    "results = distill(student_agent, teacher_agent, args={'lr':1e-2, 'iters':10000, 'lambda':5})\n",
    "\n",
    "# student_agent.save_to_file(\"deep_range_example.pt\")\n",
    "# student_agent.load_from_file(\"deep_range_example.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1: student\n",
      "Agent 2: teacher\n",
      "\n",
      "Played 2000 hands of poker.\n",
      "Player  DEEPRANGE: -136.0 +/- 157.56683117557216\n",
      "Player  AVRG_NET: 136.0 +/- 157.56683117557216\n",
      "\n",
      "\n",
      "Agent 1: student\n",
      "Agent 2: crappy\n",
      "\n",
      "Played 2000 hands of poker.\n",
      "Player  DEEPRANGE: 302.0 +/- 190.76949881451117\n",
      "Player  AVRG_NET: -302.0 +/- 190.76949881451117\n",
      "\n",
      "\n",
      "Agent 1: crappy\n",
      "Agent 2: teacher\n",
      "\n",
      "Played 2000 hands of poker.\n",
      "Player  AVRG_NET: -836.5 +/- 200.8964947712741\n",
      "Player  AVRG_NET: 836.5 +/- 200.8964947712741\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluation\n",
    "\n",
    "import time, sys\n",
    "from os.path import dirname, abspath\n",
    "\n",
    "sys.path.append(\"/home/leduc/Deep-CFR/\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from DeepCFR.EvalAgentDeepCFR import EvalAgentDeepCFR\n",
    "from PokerRL.game.AgentTournament import AgentTournament\n",
    "\n",
    "agent_file1 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/120/eval_agentAVRG_NET.pkl\"\n",
    "agent_file2 = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_2/2/eval_agentAVRG_NET.pkl\"\n",
    "\n",
    "teacher_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file1)\n",
    "crappy_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agent_file2)\n",
    "\n",
    "# student_file = \"deep_range_100000_0.01.pt\"\n",
    "# student_agent.load_from_file(student_file)\n",
    "\n",
    "def h2heval(eval_agent_1, eval_agent_2, names=['agent1', 'agent2']):\n",
    "    env_bldr = eval_agent_1.env_bldr\n",
    "    env = env_bldr.get_new_env(is_evaluating=False)\n",
    "    env_cls = env_bldr.env_cls\n",
    "    env_args = env_bldr.env_args\n",
    "    \n",
    "    print(\"Agent 1:\", names[0])\n",
    "    print(\"Agent 2:\", names[1])\n",
    "    matchup = AgentTournament(env_cls, env_args, eval_agent_1, eval_agent_2)\n",
    "    mean, upper_conf95, lower_conf95 = matchup.run(n_games_per_seat=1000)\n",
    "    print(\"\\n\")\n",
    "\n",
    "h2heval(student_agent, teacher_agent, names=['student', 'teacher'])\n",
    "h2heval(student_agent, crappy_agent, names=['student', 'crappy'])\n",
    "h2heval(crappy_agent, teacher_agent, names=['crappy', 'teacher'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
