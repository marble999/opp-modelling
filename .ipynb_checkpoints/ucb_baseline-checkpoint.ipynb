{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "\n",
    "# Compute CFR for Leduc\n",
    "# Compute EV for hands in Leduc\n",
    "# Write update equations for tournament\n",
    "# Build agents for: multi-arm-bandit,f bayesian update, bayesian optimization, range-update model, DQN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCB MAB baseline\n",
    "# UCB = empirical mean of specific arm + sqrt(2 log t/N_t), where N_t = number of trials of specific arm\n",
    "import time, sys\n",
    "from os.path import dirname, abspath\n",
    "sys.path.append(\"/home/leduc/Deep-CFR/\")\n",
    "sys.path.append(\"/home/leduc/PokerRL/\")\n",
    "import numpy as np\n",
    "from DeepCFR.EvalAgentDeepCFR import EvalAgentDeepCFR\n",
    "from PokerRL.game.AgentTournament import AgentTournament\n",
    "from H2HEvaluator import H2HEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB:    \n",
    "    \n",
    "    def __init__(self, agent_list, test_agent, gamma = 1, n_hands = 2):\n",
    "        self.agent_list = agent_list\n",
    "        self.num_agents = len(self.agent_list)\n",
    "        self.arm_times = [0 for _ in range(self.num_agents)]\n",
    "        self.ucb = [0 for _ in range(self.num_agents)]\n",
    "        self.rewards = [0 for _ in range(self.num_agents)]\n",
    "        self.test_agent = test_agent\n",
    "        self.gamma = gamma\n",
    "        self.n_hands = n_hands \n",
    "        self.reward_list = [0]\n",
    "        \n",
    "    def run(self, n_episodes):\n",
    "        nash_rewards = []\n",
    "        avg_nash_rewards = []\n",
    "        variances = [0]\n",
    "        nash_agent_path = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/120/eval_agentAVRG_NET.pkl\"\n",
    "        nash_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=nash_agent_path)\n",
    "        time = np.sum(self.arm_times)\n",
    "        \n",
    "        def ucb_updater(time, arm_time):\n",
    "            if (arm_time == 0):\n",
    "                return np.infty\n",
    "            else:\n",
    "                return np.sqrt(2 * np.log(time) / np.float(arm_time))\n",
    "            \n",
    "        for _ in range(n_episodes):\n",
    "            time += 1\n",
    "            \n",
    "            for i in range(self.num_agents):\n",
    "                self.ucb[i] = self.rewards[i] + self.gamma * ucb_updater(time, self.arm_times[i])\n",
    "            best_agent_idx = np.argmax(self.ucb)\n",
    "            \n",
    "            reward, variance = H2HEval(self.agent_list[best_agent_idx], self.test_agent).h2h_eval(self.n_hands)\n",
    "            \n",
    "            print(\"UCB List: \", self.ucb)\n",
    "            self.arm_times[best_agent_idx] += 1\n",
    "            self.rewards[best_agent_idx] = (self.rewards[best_agent_idx] * (self.arm_times[best_agent_idx] - 1) + reward) / self.arm_times[best_agent_idx]\n",
    "            print(\"best agent, reward, arm_times, avg reward\", best_agent_idx, reward,self.arm_times[best_agent_idx], self.rewards[best_agent_idx])\n",
    "            self.reward_list.append((self.reward_list[-1] * (time - 1) + reward) / time)\n",
    "            \n",
    "            nash_rewards.append(H2HEval(nash_agent, self.test_agent).h2h_eval(self.n_hands)[0])\n",
    "            avg_nash_rewards.append(np.mean(nash_rewards))\n",
    "            variances.append((variances[-1] * (time - 1) ** 2 + variance) / time ** 2)\n",
    "            \n",
    "        lower_bounds = [self.reward_list[i] - 1.96 * np.sqrt(variances[i]) for i in range(n_episodes)]\n",
    "        upper_bounds = [self.reward_list[i] + 1.96 * np.sqrt(variances[i]) for i in range(n_episodes)]\n",
    "        plt.plot(self.reward_list)\n",
    "        plt.fill_between(range(n_episodes), upper_bounds, lower_bounds, color='blue', alpha=.5)\n",
    "        plt.plot(avg_nash_rewards, color = 'red')\n",
    "        plt.title('Performance of Multiarmed Bandit (Blue) vs. Nash (Red)')\n",
    "        plt.ylabel('Average Reward Per Hand')\n",
    "        plt.xlabel('Time')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"number of trials of each arm:\", self.arm_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = fold\n",
    "# 1 = call\n",
    "# 2 = raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PokerRL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0fdadd60d2e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPokerRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgames\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardLeduc\u001b[0m  \u001b[0;31m# or any other game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPokerRL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalAgentBase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvalAgentBase\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_EvalAgentBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDeepCFR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalAgentDeepCFR\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvalAgentDeepCFR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDeepCFR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingProfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingProfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDeepCFR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDriver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PokerRL'"
     ]
    }
   ],
   "source": [
    "from PokerRL.game.games import StandardLeduc  # or any other game\n",
    "\n",
    "from DeepCFR.EvalAgentDeepCFR import EvalAgentDeepCFR\n",
    "from DeepCFR.TrainingProfile import TrainingProfile\n",
    "from DeepCFR.workers.driver.Driver import Driver\n",
    "\n",
    "t_prof = TrainingProfile(name=\"MAB\",\n",
    "                                         nn_type=\"feedforward\",\n",
    "                                         max_buffer_size_adv=3e6,\n",
    "                                         eval_agent_export_freq=20,  # export API to play against the agent\n",
    "                                         n_traversals_per_iter=1500,\n",
    "                                         n_batches_adv_training=750,\n",
    "                                         n_batches_avrg_training=2000,\n",
    "                                         n_merge_and_table_layer_units_adv=64,\n",
    "                                         n_merge_and_table_layer_units_avrg=64,\n",
    "                                         n_units_final_adv=64,\n",
    "                                         n_units_final_avrg=64,\n",
    "                                         mini_batch_size_adv=2048,\n",
    "                                         mini_batch_size_avrg=2048,\n",
    "                                         init_adv_model=\"last\",\n",
    "                                         init_avrg_model=\"last\",\n",
    "                                         use_pre_layers_adv=False,\n",
    "                                         use_pre_layers_avrg=False,\n",
    "\n",
    "                                         game_cls=StandardLeduc,\n",
    "\n",
    "                                         # You can specify one or both modes. Choosing both is useful to compare them.\n",
    "                                         eval_modes_of_algo=(\n",
    "                                             EvalAgentDeepCFR.EVAL_MODE_SINGLE,  # SD-CFR\n",
    "                                             EvalAgentDeepCFR.EVAL_MODE_AVRG_NET,  # Deep CFR\n",
    "                                         ),\n",
    "\n",
    "                                         DISTRIBUTED=False,\n",
    "                                         )\n",
    "fold_agent = FoldAgent(t_prof=t_prof)\n",
    "bet_agent = BetAgent(t_prof=t_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/20/eval_agentAVRG_NET.pkl', '/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/40/eval_agentAVRG_NET.pkl', '/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/60/eval_agentAVRG_NET.pkl', '/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/80/eval_agentAVRG_NET.pkl', '/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/100/eval_agentAVRG_NET.pkl', '/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/20/eval_agentSINGLE.pkl', '/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/40/eval_agentSINGLE.pkl', '/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/60/eval_agentSINGLE.pkl', '/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/80/eval_agentSINGLE.pkl', '/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/100/eval_agentSINGLE.pkl'] bet agent\n"
     ]
    }
   ],
   "source": [
    "avg_agents = [\"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/\" + str(20 * i) + \"/eval_agentAVRG_NET.pkl\" for i in range(1,6)]\n",
    "single_agents = [\"/home/leduc/poker_Å¾ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_200/\" + str(20 * i) + \"/eval_agentSINGLE.pkl\" for i in range(1,6)]\n",
    "agents = avg_agents + single_agents\n",
    "print(agents, \"bet agent\")\n",
    "agents = [EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=agents[i]) for i in range(len(agents))]\n",
    "agents.append(bet_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ee0727ea18d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrandom_agent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_2/2/eval_agentAVRG_NET.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrandom_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalAgentDeepCFR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_eval_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_agent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agents' is not defined"
     ]
    }
   ],
   "source": [
    "agents = agents[:-1]\n",
    "random_agent_path = \"/home/leduc/poker_ai_data/eval_agent/SD-CFR_LEDUC_EXAMPLE_2/2/eval_agentAVRG_NET.pkl\"\n",
    "random_agent = EvalAgentDeepCFR.load_from_disk(path_to_eval_agent=random_agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit = MAB(agents, random_agent, n_hands = 50)\n",
    "bandit.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = MAB(agents, random_agent, gamma = 0.5, n_hands = 10)\n",
    "bandit.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = MAB(agents, random_agent, gamma = 10, n_hands = 10)\n",
    "bandit.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
