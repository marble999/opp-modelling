{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO \n",
    "\n",
    "# Compute CFR for Leduc\n",
    "# Compute EV for hands in Leduc\n",
    "# Write update equations for tournament\n",
    "# Build agents for: multi-arm-bandit, bayesian update, bayesian optimization, range-update model, DQN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with stack size [20000, 20000] has 1095 nodes out of which 428 are non-terminal.\n",
      "Iteration:  0\n",
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n",
      "Iteration:  10\n",
      "Iteration:  11\n",
      "Iteration:  12\n",
      "Iteration:  13\n",
      "Iteration:  14\n",
      "Iteration:  15\n",
      "Iteration:  16\n",
      "Iteration:  17\n",
      "Iteration:  18\n",
      "Iteration:  19\n",
      "Iteration:  20\n",
      "Iteration:  21\n",
      "Iteration:  22\n",
      "Iteration:  23\n",
      "Iteration:  24\n",
      "Iteration:  25\n",
      "Iteration:  26\n",
      "Iteration:  27\n",
      "Iteration:  28\n",
      "Iteration:  29\n",
      "Iteration:  30\n",
      "Iteration:  31\n",
      "Iteration:  32\n",
      "Iteration:  33\n",
      "Iteration:  34\n",
      "Iteration:  35\n",
      "Iteration:  36\n",
      "Iteration:  37\n",
      "Iteration:  38\n",
      "Iteration:  39\n",
      "Iteration:  40\n",
      "Iteration:  41\n",
      "Iteration:  42\n",
      "Iteration:  43\n",
      "Iteration:  44\n",
      "Iteration:  45\n",
      "Iteration:  46\n",
      "Iteration:  47\n",
      "Iteration:  48\n",
      "Iteration:  49\n",
      "Iteration:  50\n",
      "Iteration:  51\n",
      "Iteration:  52\n",
      "Iteration:  53\n",
      "Iteration:  54\n",
      "Iteration:  55\n",
      "Iteration:  56\n",
      "Iteration:  57\n",
      "Iteration:  58\n",
      "Iteration:  59\n",
      "Iteration:  60\n",
      "Iteration:  61\n",
      "Iteration:  62\n",
      "Iteration:  63\n",
      "Iteration:  64\n",
      "Iteration:  65\n",
      "Iteration:  66\n",
      "Iteration:  67\n",
      "Iteration:  68\n",
      "Iteration:  69\n",
      "Iteration:  70\n",
      "Iteration:  71\n",
      "Iteration:  72\n",
      "Iteration:  73\n",
      "Iteration:  74\n",
      "Iteration:  75\n",
      "Iteration:  76\n",
      "Iteration:  77\n",
      "Iteration:  78\n",
      "Iteration:  79\n",
      "Iteration:  80\n",
      "Iteration:  81\n",
      "Iteration:  82\n",
      "Iteration:  83\n",
      "Iteration:  84\n",
      "Iteration:  85\n",
      "Iteration:  86\n",
      "Iteration:  87\n",
      "Iteration:  88\n",
      "Iteration:  89\n",
      "Iteration:  90\n",
      "Iteration:  91\n",
      "Iteration:  92\n",
      "Iteration:  93\n",
      "Iteration:  94\n",
      "Iteration:  95\n",
      "Iteration:  96\n",
      "Iteration:  97\n",
      "Iteration:  98\n",
      "Iteration:  99\n",
      "Iteration:  100\n",
      "Iteration:  101\n",
      "Iteration:  102\n",
      "Iteration:  103\n",
      "Iteration:  104\n",
      "Iteration:  105\n",
      "Iteration:  106\n",
      "Iteration:  107\n",
      "Iteration:  108\n",
      "Iteration:  109\n",
      "Iteration:  110\n",
      "Iteration:  111\n",
      "Iteration:  112\n",
      "Iteration:  113\n",
      "Iteration:  114\n",
      "Iteration:  115\n",
      "Iteration:  116\n",
      "Iteration:  117\n",
      "Iteration:  118\n",
      "Iteration:  119\n",
      "Iteration:  120\n",
      "Iteration:  121\n",
      "Iteration:  122\n",
      "Iteration:  123\n",
      "Iteration:  124\n",
      "Iteration:  125\n",
      "Iteration:  126\n",
      "Iteration:  127\n",
      "Iteration:  128\n",
      "Iteration:  129\n",
      "Iteration:  130\n",
      "Iteration:  131\n",
      "Iteration:  132\n",
      "Iteration:  133\n",
      "Iteration:  134\n",
      "Iteration:  135\n",
      "Iteration:  136\n",
      "Iteration:  137\n",
      "Iteration:  138\n",
      "Iteration:  139\n",
      "Iteration:  140\n",
      "Iteration:  141\n",
      "Iteration:  142\n",
      "Iteration:  143\n",
      "Iteration:  144\n",
      "Iteration:  145\n",
      "Iteration:  146\n",
      "Iteration:  147\n",
      "Iteration:  148\n",
      "Iteration:  149\n"
     ]
    }
   ],
   "source": [
    "from PokerRL.cfr.VanillaCFR import VanillaCFR\n",
    "from PokerRL.game import bet_sets\n",
    "from PokerRL.game.games import DiscretizedNLLeduc\n",
    "from PokerRL.rl.base_cls.workers.ChiefBase import ChiefBase\n",
    "\n",
    "from PokerRL._.CrayonWrapper import CrayonWrapper\n",
    "\n",
    "n_iterations = 150\n",
    "name = \"CFR_EXAMPLE\"\n",
    "\n",
    "# Passing None for t_prof will is enough for ChiefBase. We only use it to log; This CFR impl is not distributed.\n",
    "chief = ChiefBase(t_prof=None)\n",
    "crayon = CrayonWrapper(name=name,\n",
    "                       path_log_storage=None,\n",
    "                       chief_handle=chief,\n",
    "                       runs_distributed=False,\n",
    "                       runs_cluster=False,\n",
    "                       )\n",
    "cfr = VanillaCFR(name=name,\n",
    "                 game_cls=DiscretizedNLLeduc,\n",
    "                 agent_bet_set=bet_sets.POT_ONLY,\n",
    "                 chief_handle=chief)\n",
    "\n",
    "for iter_id in range(n_iterations):\n",
    "    print(\"Iteration: \", iter_id)\n",
    "    cfr.iteration()\n",
    "    crayon.update_from_log_buffer()\n",
    "    crayon.export_all(iter_nr=iter_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PokerRL.cfr.VanillaCFR.VanillaCFR at 0x7fe22bcfe630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PokerRL.game._.tree.PublicTree.PublicTree at 0x7fe284d5e828>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfr._trees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from PokerRL.game import Poker\n",
    "from PokerRL.game._.tree._.nodes import PlayerActionNode\n",
    "from PokerRL.rl import rl_util\n",
    "from PokerRL.rl.base_cls.EvalAgentBase import EvalAgentBase as _EvalAgentBase\n",
    "from PokerRL.rl.errors import UnknownModeError\n",
    "\n",
    "from DeepCFR.IterationStrategy import IterationStrategy\n",
    "from DeepCFR.StrategyBuffer import StrategyBuffer\n",
    "from DeepCFR.workers.la.AvrgWrapper import AvrgWrapper\n",
    "\n",
    "NP_FLOAT_TYPE = np.float64  # Use 64 for extra stability in big games\n",
    "\n",
    "\n",
    "class EvalAgentFromTree(_EvalAgentBase):\n",
    "    \n",
    "    ALL_MODES = [\"EVAL\"]\n",
    "\n",
    "    def __init__(self, t_prof, tree, mode=None, device=None):\n",
    "        super().__init__(t_prof=t_prof, mode=mode, device=device)\n",
    "        self.tree = tree\n",
    "\n",
    "    def can_compute_mode(self):\n",
    "        \"\"\" All modes are always computable (i.e. not dependent on iteration etc.)\"\"\"\n",
    "        return True\n",
    "\n",
    "    def get_a_probs_for_each_hand(self):\n",
    "        \"\"\" BEFORE CALLING, NOTIFY EVALAGENT OF THE PAST ACTIONS / ACTIONSEQUENCE!!!!! \"\"\"\n",
    "        pub_obs = self._internal_env_wrapper.get_current_obs()\n",
    "        legal_actions_list = self._internal_env_wrapper.env.get_legal_actions()\n",
    "        p_id_acting = self._internal_env_wrapper.env.current_player.seat_id\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # Deep CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        if self._mode == self.EVAL_MODE_AVRG_NET:\n",
    "            return self.avrg_net_policies[p_id_acting].get_a_probs_for_each_hand(pub_obs=pub_obs,\n",
    "                                                                                 legal_actions_list=legal_actions_list)\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # SD-CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        elif self._mode == self.EVAL_MODE_SINGLE:\n",
    "\n",
    "            unif_rand_legal = np.full(\n",
    "                shape=self.env_bldr.N_ACTIONS,\n",
    "                fill_value=1.0 / len(legal_actions_list)\n",
    "            ) * rl_util.get_legal_action_mask_np(n_actions=self.env_bldr.N_ACTIONS,\n",
    "                                                 legal_actions_list=legal_actions_list,\n",
    "                                                 dtype=np.float32)\n",
    "\n",
    "            n_models = self._strategy_buffers[p_id_acting].size\n",
    "            if n_models == 0:\n",
    "                return np.repeat(np.expand_dims(unif_rand_legal, axis=0),\n",
    "                                 repeats=self.env_bldr.rules.RANGE_SIZE, axis=0)\n",
    "            else:\n",
    "                # Dim: [model_idx, range_idx]\n",
    "                reaches = self._get_reach_for_each_model_each_hand(p_id_acting=p_id_acting)\n",
    "\n",
    "                # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                # Compute strategy for\n",
    "                # all infosets with\n",
    "                # reach >0. Initialize\n",
    "                # All others stay unif.\n",
    "                # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                contrib_each_model = np.zeros(\n",
    "                    shape=(n_models, self.env_bldr.rules.RANGE_SIZE, self.env_bldr.N_ACTIONS),\n",
    "                    dtype=NP_FLOAT_TYPE\n",
    "                )\n",
    "\n",
    "                for m_i, (strat, weight) in enumerate(self._strategy_buffers[p_id_acting].get_strats_and_weights()):\n",
    "                    range_idxs = np.nonzero(reaches[m_i])[0]\n",
    "                    if range_idxs.shape[0] > 0:\n",
    "                        a_probs_m = strat.get_a_probs_for_each_hand_in_list(\n",
    "                            pub_obs=pub_obs,\n",
    "                            range_idxs=range_idxs,\n",
    "                            legal_actions_list=legal_actions_list\n",
    "                        )\n",
    "                        contrib_each_model[m_i, range_idxs] = a_probs_m * weight\n",
    "\n",
    "                # Dim: [range_idx, action_p]\n",
    "                a_probs = (np.sum(contrib_each_model * np.expand_dims(reaches, axis=2), axis=0)).astype(NP_FLOAT_TYPE)\n",
    "\n",
    "                # Dim: [range_idx]\n",
    "                a_probs_sum = np.expand_dims(np.sum(a_probs, axis=1), axis=1)\n",
    "\n",
    "                # Dim: [range_idx, action_p]\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    return np.where(a_probs_sum == 0,\n",
    "                                    np.repeat(np.expand_dims(unif_rand_legal, axis=0),\n",
    "                                              repeats=self._internal_env_wrapper.env.RANGE_SIZE, axis=0),\n",
    "                                    a_probs / a_probs_sum\n",
    "                                    )\n",
    "\n",
    "        else:\n",
    "            raise UnknownModeError(self._mode)\n",
    "\n",
    "    def get_a_probs(self):\n",
    "        pub_obs = self._internal_env_wrapper.get_current_obs()\n",
    "        legal_actions_list = self._internal_env_wrapper.env.get_legal_actions()\n",
    "        p_id_acting = self._internal_env_wrapper.env.current_player.seat_id\n",
    "        range_idx = self._internal_env_wrapper.env.get_range_idx(p_id=p_id_acting)\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # Deep CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        if self._mode == self.EVAL_MODE_AVRG_NET:\n",
    "            return self.avrg_net_policies[p_id_acting].get_a_probs(\n",
    "                pub_obses=[pub_obs],\n",
    "                range_idxs=np.array([range_idx], dtype=np.int32),\n",
    "                legal_actions_lists=[legal_actions_list]\n",
    "            )[0]\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # SD-CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        elif self._mode == self.EVAL_MODE_SINGLE:\n",
    "\n",
    "            if self._strategy_buffers[p_id_acting].size == 0:\n",
    "                unif_rand_legal = np.full(\n",
    "                    shape=self.env_bldr.N_ACTIONS,\n",
    "                    fill_value=1.0 / len(legal_actions_list)\n",
    "                ) * rl_util.get_legal_action_mask_np(n_actions=self.env_bldr.N_ACTIONS,\n",
    "                                                     legal_actions_list=legal_actions_list,\n",
    "                                                     dtype=np.float32)\n",
    "                return unif_rand_legal\n",
    "            else:\n",
    "                # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                # Weighted by Iteration\n",
    "                # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                # Dim: [model_idx, action_p]\n",
    "                a_probs_each_model = np.array([\n",
    "                    weight * strat.get_a_probs(pub_obses=[pub_obs],\n",
    "                                               range_idxs=[range_idx],\n",
    "                                               legal_actions_lists=[legal_actions_list]\n",
    "                                               )[0]\n",
    "                    for strat, weight in self._strategy_buffers[p_id_acting].get_strats_and_weights()\n",
    "                ])\n",
    "\n",
    "                # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                # Weighted by Reach\n",
    "                # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                a_probs_each_model *= np.expand_dims(self._get_reach_for_each_model(\n",
    "                    p_id_acting=p_id_acting,\n",
    "                    range_idx=range_idx,\n",
    "                ), axis=2)\n",
    "\n",
    "                # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                # Normalize\n",
    "                # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                # Dim: [action_p]\n",
    "                a_probs = np.sum(a_probs_each_model, axis=0)\n",
    "\n",
    "                # Dim: []\n",
    "                a_probs_sum = np.sum(a_probs)\n",
    "\n",
    "                # Dim: [action_p]\n",
    "                return a_probs / a_probs_sum\n",
    "\n",
    "        else:\n",
    "            raise UnknownModeError(self._mode)\n",
    "\n",
    "    def get_action(self, step_env=True, need_probs=False):\n",
    "        \"\"\" !! BEFORE CALLING, NOTIFY EVALAGENT OF THE PAST ACTIONS / ACTIONSEQUENCE !! \"\"\"\n",
    "\n",
    "        p_id_acting = self._internal_env_wrapper.env.current_player.seat_id\n",
    "        range_idx = self._internal_env_wrapper.env.get_range_idx(p_id=p_id_acting)\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # Deep CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        if self._mode == self.EVAL_MODE_AVRG_NET:\n",
    "            if need_probs:  # only do if necessary\n",
    "                a_probs_all_hands = self.get_a_probs_for_each_hand()\n",
    "                a_probs = a_probs_all_hands[range_idx]\n",
    "            else:\n",
    "                a_probs_all_hands = None  # not needed\n",
    "\n",
    "                a_probs = self.avrg_net_policies[p_id_acting].get_a_probs(\n",
    "                    pub_obses=[self._internal_env_wrapper.get_current_obs()],\n",
    "                    range_idxs=np.array([range_idx], dtype=np.int32),\n",
    "                    legal_actions_lists=[self._internal_env_wrapper.env.get_legal_actions()]\n",
    "                )[0]\n",
    "\n",
    "            action = np.random.choice(np.arange(self.env_bldr.N_ACTIONS), p=a_probs)\n",
    "\n",
    "            if step_env:\n",
    "                self._internal_env_wrapper.step(action=action)\n",
    "\n",
    "            return action, a_probs_all_hands\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # SD-CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        elif self._mode == self.EVAL_MODE_SINGLE:\n",
    "            if need_probs:\n",
    "                a_probs_all_hands = self.get_a_probs_for_each_hand()\n",
    "            else:\n",
    "                a_probs_all_hands = None  # not needed\n",
    "\n",
    "            legal_actions_list = self._internal_env_wrapper.env.get_legal_actions()\n",
    "\n",
    "            if self._episode_net_idxs[p_id_acting] is None:  # Iteration 0\n",
    "                action = legal_actions_list[np.random.randint(len(legal_actions_list))]\n",
    "            else:  # Iteration > 0\n",
    "                action = self._strategy_buffers[p_id_acting].get(self._episode_net_idxs[p_id_acting]).get_action(\n",
    "                    pub_obses=[self._internal_env_wrapper.get_current_obs()],\n",
    "                    range_idxs=[range_idx],\n",
    "                    legal_actions_lists=[legal_actions_list],\n",
    "                )[0].item()\n",
    "\n",
    "            if step_env:\n",
    "                # add to history before modifying env state\n",
    "                self._add_history_entry(p_id_acting=p_id_acting, action_hes_gonna_do=action)\n",
    "\n",
    "                # make INTERNAL step to keep up with the game state.\n",
    "                self._internal_env_wrapper.step(action=action)\n",
    "\n",
    "            return action, a_probs_all_hands\n",
    "        else:\n",
    "            raise UnknownModeError(self._mode)\n",
    "\n",
    "    def get_action_frac_tuple(self, step_env):\n",
    "        a_idx_raw = self.get_action(step_env=step_env, need_probs=False)[0]\n",
    "\n",
    "        if self.env_bldr.env_cls.IS_FIXED_LIMIT_GAME:\n",
    "            return a_idx_raw, -1\n",
    "        else:\n",
    "            if a_idx_raw >= 2:\n",
    "                frac = self.env_bldr.env_args.bet_sizes_list_as_frac_of_pot[a_idx_raw - 2]\n",
    "                return [Poker.BET_RAISE, frac]\n",
    "            return [a_idx_raw, -1]\n",
    "\n",
    "    def update_weights(self, weights_for_eval_agent):\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # Deep CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        if self._AVRG:\n",
    "            avrg_weights = weights_for_eval_agent[self.EVAL_MODE_AVRG_NET]\n",
    "\n",
    "            for p in range(self.t_prof.n_seats):\n",
    "                self.avrg_net_policies[p].load_net_state_dict(self.ray.state_dict_to_torch(avrg_weights[p],\n",
    "                                                                                           device=self.device))\n",
    "                self.avrg_net_policies[p].eval()\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # SD-CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        if self._SINGLE:\n",
    "            list_of_new_iter_strat_state_dicts = copy.deepcopy(weights_for_eval_agent[self.EVAL_MODE_SINGLE])\n",
    "\n",
    "            for p in range(self.t_prof.n_seats):\n",
    "                for state in list_of_new_iter_strat_state_dicts[p]:\n",
    "                    state[\"net\"] = self.ray.state_dict_to_torch(state[\"net\"], device=self.device)\n",
    "\n",
    "                    _iter_strat = IterationStrategy.build_from_state_dict(state=state, t_prof=self.t_prof,\n",
    "                                                                          env_bldr=self.env_bldr,\n",
    "                                                                          device=self.device)\n",
    "\n",
    "                    self._strategy_buffers[p].add(iteration_strat=_iter_strat)\n",
    "\n",
    "    def _state_dict(self):\n",
    "        d = {}\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # Deep CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        if self._AVRG:\n",
    "            d[\"avrg_nets\"] = [pol.net_state_dict() for pol in self.avrg_net_policies]\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # SD-CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        if self._SINGLE:\n",
    "            d[\"strategy_buffers\"] = [self._strategy_buffers[p].state_dict() for p in range(self.t_prof.n_seats)]\n",
    "            d[\"curr_net_idxs\"] = copy.deepcopy(self._episode_net_idxs)\n",
    "            d[\"history\"] = copy.deepcopy(self._a_history)\n",
    "\n",
    "        return d\n",
    "\n",
    "    def _load_state_dict(self, state):\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # Deep CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        if self._AVRG:\n",
    "            for i in range(self.t_prof.n_seats):\n",
    "                self.avrg_net_policies[i].load_net_state_dict(state[\"avrg_nets\"][i])\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # SD-CFR\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        if self._SINGLE:\n",
    "            for p in range(self.t_prof.n_seats):\n",
    "                self._strategy_buffers[p].load_state_dict(state=state[\"strategy_buffers\"][p])\n",
    "            self._a_history = copy.deepcopy(state['history'])\n",
    "            self._episode_net_idxs = copy.deepcopy(state['curr_net_idxs'])\n",
    "\n",
    "    # _____________________________________________ SD-CFR specific _____________________________________________\n",
    "    def _add_history_entry(self, p_id_acting, action_hes_gonna_do):\n",
    "        self._a_history[p_id_acting][\"pub_obs_batch\"].append(self._internal_env_wrapper.get_current_obs())\n",
    "        self._a_history[p_id_acting][\"legal_action_list_batch\"].append(\n",
    "            self._internal_env_wrapper.env.get_legal_actions())\n",
    "        self._a_history[p_id_acting][\"a_batch\"].append(action_hes_gonna_do)\n",
    "        self._a_history[p_id_acting][\"len\"] += 1\n",
    "\n",
    "    def _get_reach_for_each_model(self, p_id_acting, range_idx):\n",
    "        models = self._strategy_buffers[p_id_acting].strategies\n",
    "\n",
    "        H = self._a_history[p_id_acting]\n",
    "        if H['len'] == 0:\n",
    "            # Dim: [model_idx]\n",
    "            return np.ones(shape=(len(models)), dtype=np.float32)\n",
    "\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # Batch calls history\n",
    "        # and computes product\n",
    "        # of result\n",
    "        # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "        # Dim: [model_idx, history_time_step]\n",
    "        prob_a_each_model_each_timestep = np.array(\n",
    "            [\n",
    "                model.get_a_probs(\n",
    "                    pub_obses=H['pub_obs_batch'],\n",
    "                    range_idxs=[range_idx] * H['len'],\n",
    "                    legal_actions_lists=H['legal_action_list_batch'],\n",
    "                )[np.arange(len(models)), H['a_batch']]\n",
    "\n",
    "                for model in models\n",
    "            ]\n",
    "        )\n",
    "        # Dim: [model_idx]\n",
    "        return np.prod(a=prob_a_each_model_each_timestep, axis=1)\n",
    "\n",
    "    def _get_reach_for_each_model_each_hand(self, p_id_acting):\n",
    "        # Probability that each model would perform action a (from history) with each hand\n",
    "        models = self._strategy_buffers[p_id_acting].strategies\n",
    "\n",
    "        # Dim: [model_idx, range_idx]\n",
    "        reaches = np.empty(shape=(len(models), self.env_bldr.rules.RANGE_SIZE,), dtype=NP_FLOAT_TYPE)\n",
    "\n",
    "        H = self._a_history[p_id_acting]\n",
    "\n",
    "        for m_i, model in enumerate(models):\n",
    "            non_zero_hands = list(range(self.env_bldr.rules.RANGE_SIZE))\n",
    "\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            # Batch calls hands but\n",
    "            # not history timesteps.\n",
    "            # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            reach_hist = np.zeros(shape=(H['len'], self.env_bldr.rules.RANGE_SIZE), dtype=NP_FLOAT_TYPE)\n",
    "            for hist_idx in range(H['len']):\n",
    "                if len(non_zero_hands) == 0:\n",
    "                    break\n",
    "\n",
    "                # Dim: [model_idx, RANGE_SIZE]\n",
    "                p_m_a = model.get_a_probs_for_each_hand_in_list(\n",
    "                    pub_obs=H['pub_obs_batch'][hist_idx],\n",
    "                    legal_actions_list=H['legal_action_list_batch'][hist_idx],\n",
    "                    range_idxs=np.array(non_zero_hands),\n",
    "                )[:, H['a_batch'][hist_idx]]\n",
    "\n",
    "                reach_hist[hist_idx, non_zero_hands] = p_m_a * len(H['legal_action_list_batch'][hist_idx])\n",
    "                # collect zeros to avoid unnecessary future queries\n",
    "                for h_idx in reversed(range(len(non_zero_hands))):\n",
    "                    if p_m_a[h_idx] == 0:\n",
    "                        del non_zero_hands[h_idx]\n",
    "\n",
    "            reaches[m_i] = np.prod(reach_hist, axis=0)\n",
    "\n",
    "        # Dim: [model_idx, RANGE_SIZE]\n",
    "        return reaches\n",
    "\n",
    "    def _sample_new_strategy(self):\n",
    "        \"\"\"\n",
    "        Sample one current strategy from the buffer to play by this episode\n",
    "        \"\"\"\n",
    "        self._episode_net_idxs = [\n",
    "            self._strategy_buffers[p].sample_strat_idx_weighted()\n",
    "            for p in range(self.env_bldr.N_SEATS)\n",
    "        ]\n",
    "\n",
    "    def _reset_action_history(self):\n",
    "        self._a_history = {\n",
    "            p_id: {\n",
    "                \"pub_obs_batch\": [],\n",
    "                \"legal_action_list_batch\": [],\n",
    "                \"a_batch\": [],\n",
    "                \"len\": 0,\n",
    "            }\n",
    "            for p_id in range(self.env_bldr.N_SEATS)\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
